{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cheongwoong/miniconda3/envs/factual_knowledge_probing/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import torch\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/analogy'\n",
    "valid_path = os.path.join(data_path, 'valid.json')\n",
    "test_path = os.path.join(data_path, 'test.json')\n",
    "valid_data = json.load(open(valid_path, 'r'))\n",
    "test_data = json.load(open(test_path, 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-12-02 16:08:41,016] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'bert.pooler.dense.bias', 'cls.seq_relationship.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 13656/13656 [03:14<00:00, 70.27it/s]\n",
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'bert.pooler.dense.bias', 'cls.seq_relationship.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 13656/13656 [05:04<00:00, 44.91it/s]\n",
      "100%|██████████| 13656/13656 [03:22<00:00, 67.35it/s]\n",
      "100%|██████████| 13656/13656 [05:18<00:00, 42.92it/s]\n",
      "Some weights of the model checkpoint at albert-base-v1 were not used when initializing AlbertForMaskedLM: ['albert.pooler.bias', 'albert.pooler.weight']\n",
      "- This IS expected if you are initializing AlbertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 13656/13656 [03:12<00:00, 70.96it/s]\n",
      "Some weights of the model checkpoint at albert-large-v1 were not used when initializing AlbertForMaskedLM: ['albert.pooler.bias', 'albert.pooler.weight']\n",
      "- This IS expected if you are initializing AlbertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 13656/13656 [05:01<00:00, 45.35it/s]\n",
      "Some weights of the model checkpoint at albert-xlarge-v1 were not used when initializing AlbertForMaskedLM: ['albert.pooler.bias', 'albert.pooler.weight']\n",
      "- This IS expected if you are initializing AlbertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 13656/13656 [09:52<00:00, 23.05it/s]\n",
      "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForMaskedLM: ['albert.pooler.bias', 'albert.pooler.weight']\n",
      "- This IS expected if you are initializing AlbertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 13656/13656 [03:32<00:00, 64.40it/s]\n",
      "Some weights of the model checkpoint at albert-large-v2 were not used when initializing AlbertForMaskedLM: ['albert.pooler.bias', 'albert.pooler.weight']\n",
      "- This IS expected if you are initializing AlbertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 13656/13656 [05:27<00:00, 41.65it/s]\n",
      "Some weights of the model checkpoint at albert-xlarge-v2 were not used when initializing AlbertForMaskedLM: ['albert.pooler.bias', 'albert.pooler.weight']\n",
      "- This IS expected if you are initializing AlbertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 13656/13656 [10:08<00:00, 22.45it/s]\n"
     ]
    }
   ],
   "source": [
    "model_names = ['bert-base-uncased', 'bert-large-uncased',\n",
    "                 'roberta-base', 'roberta-large',\n",
    "                 'albert-base-v1', 'albert-large-v1', 'albert-xlarge-v1',\n",
    "                 'albert-base-v2', 'albert-large-v2', 'albert-xlarge-v2',] \n",
    "\n",
    "for model_name in model_names:\n",
    "    # Load model and tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForMaskedLM.from_pretrained(model_name).cuda()\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for sample in tqdm(test_data):\n",
    "        uid = sample['uid']\n",
    "        sentence = sample['input']\n",
    "\n",
    "        # Tokenize the input sentence\n",
    "        tokens = tokenizer.tokenize(sentence)\n",
    "        end_tail = len(tokens) - 2\n",
    "        tail = tokenizer.tokenize(sample['output'])\n",
    "        start_tail = end_tail - len(tail) + 1\n",
    "        \n",
    "        input_ids_list = []\n",
    "        masked_indices = []\n",
    "\n",
    "        # Create a list of inputs with each token masked one at a time\n",
    "        for i in range(len(tokens)):\n",
    "            masked_tokens = tokens[:]\n",
    "            masked_tokens[i] = tokenizer.mask_token\n",
    "            masked_input = tokenizer.convert_tokens_to_string(masked_tokens)\n",
    "            inputs = tokenizer(masked_input, return_tensors=\"pt\").to('cuda')\n",
    "            input_ids_list.append(inputs[\"input_ids\"][0])\n",
    "            masked_indices.append((inputs[\"input_ids\"] == tokenizer.mask_token_id).nonzero(as_tuple=True)[1])\n",
    "\n",
    "        # Pad input IDs to the same length\n",
    "        input_ids_batched = torch.nn.utils.rnn.pad_sequence(input_ids_list, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "        \n",
    "        # Create attention masks\n",
    "        attention_masks = (input_ids_batched != tokenizer.pad_token_id).long()\n",
    "\n",
    "        # Get model predictions in a single batched inference\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids_batched, attention_mask=attention_masks)\n",
    "            logits = outputs.logits\n",
    "\n",
    "        # Calculate probabilities for each masked token\n",
    "        total_log_prob = 0\n",
    "        tail_log_prob = 0\n",
    "        for i, token in enumerate(tokens):\n",
    "            softmax = torch.nn.functional.softmax(logits[i, masked_indices[i], :], dim=-1)\n",
    "            token_id = tokenizer.convert_tokens_to_ids(token)\n",
    "            word_prob = softmax[0, token_id].item()\n",
    "            total_log_prob += math.log(word_prob)\n",
    "            if start_tail <= i <= end_tail:\n",
    "                tail_log_prob += math.log(word_prob)\n",
    "\n",
    "        # Calculate perplexity\n",
    "        avg_log_prob = total_log_prob / len(tokens)\n",
    "        avg_tail_log_prob = tail_log_prob / len(tail)\n",
    "        perplexity = math.exp(-avg_log_prob)\n",
    "        tail_ppl = math.exp(-avg_tail_log_prob)\n",
    "\n",
    "        result = {'uid': uid, 'ppl': perplexity, 'tail_ppl': tail_ppl}\n",
    "        results.append(result)\n",
    "\n",
    "    os.makedirs('results', exist_ok=True)\n",
    "    output_model_name = model_name.split('/')[-1]\n",
    "    with open(os.path.join('results', f'{output_model_name}.json'), 'w') as fout:\n",
    "        json.dump(results, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "factual_knowledge_probing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
