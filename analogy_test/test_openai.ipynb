{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "from openai import OpenAI\n",
    "import tiktoken\n",
    "\n",
    "import json\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = ''\n",
    "\n",
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=OPENAI_API_KEY,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/analogy'\n",
    "test_path = os.path.join(data_path, 'MC_test.json')\n",
    "test_data = json.load(open(test_path, 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out invalid examples (whose answers are not in the ChatGPT's vocabulary)\n",
    "uids = []\n",
    "prompts = []\n",
    "\n",
    "for example in tqdm(test_data):\n",
    "    uid = example['uid']\n",
    "    prompt = example['input']\n",
    "    prompt = f' {prompt} OUTPUT ONLY LETTER.'\n",
    "\n",
    "    if True: # there is no filtering rule for this dataset\n",
    "        uids.append(uid)\n",
    "        prompts.append(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run completions with API and store the results\n",
    "raw_predictions = []\n",
    "raw_predictions_remove_stopwords = []\n",
    "\n",
    "batch_size = 1 # chatgpt does not support batched completion for now\n",
    "for i in tqdm(range(0, len(prompts), batch_size)):\n",
    "    uid_batch = uids[i:i+batch_size]\n",
    "    prompt_batch = prompts[i:i+batch_size]\n",
    "    messages = []\n",
    "    for prompt in prompt_batch:\n",
    "        messages.append({\"role\": \"user\", \"content\": prompt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_model = 'gpt-3.5-turbo-0125'\n",
    "# target_model = 'gpt-4o-2024-08-06'\n",
    "\n",
    "target_model = 'gpt-4o-2024-08-06'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run completions with API and store the results\n",
    "raw_predictions = []\n",
    "raw_predictions_remove_stopwords = []\n",
    "\n",
    "batch_size = 1 # chatgpt does not support batched completion for now\n",
    "for i in tqdm(range(0, len(prompts), batch_size)):\n",
    "    uid_batch = uids[i:i+batch_size]\n",
    "    prompt_batch = prompts[i:i+batch_size]\n",
    "    messages = []\n",
    "    for prompt in prompt_batch:\n",
    "        messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            # responses = client.chat.completions.create(\n",
    "\t\t\t# \tmodel=args.target_model,\n",
    "\t\t\t# \tmessages=messages,\n",
    "\t\t\t# \tmax_tokens=1,\n",
    "\t\t\t# \ttemperature=0,\n",
    "            #     logprobs=True,\n",
    "            #     top_logprobs=20,\n",
    "\t\t\t# )\n",
    "\n",
    "            responses_remove_stopwords = client.chat.completions.create(\n",
    "\t\t\t\tmodel=target_model,\n",
    "\t\t\t\tmessages=messages,\n",
    "\t\t\t\tmax_tokens=1,\n",
    "\t\t\t\ttemperature=0,\n",
    "                logprobs=True,\n",
    "                top_logprobs=20,\n",
    "\t\t\t)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print('Error!', e)\n",
    "            time.sleep(3)\n",
    "\n",
    "    # for uid, response in zip(uid_batch, responses.choices):\n",
    "    #     raw_predictions.append({\"uid\": uid, \"response\": response})\n",
    "    # for uid, response_remove_stopwords in zip(uid_batch, responses_remove_stopwords.choices):\n",
    "    #     raw_predictions_remove_stopwords.append({\"uid\": uid, \"response\": response_remove_stopwords})\n",
    "    uid = uid_batch[0]\n",
    "    logprobs_remove_stopwords = responses_remove_stopwords.choices[0].logprobs.content[0].top_logprobs\n",
    "    top_k_tokens_remove_stopwords, top_k_logprobs_remove_stopwords = [], []\n",
    "    for logprob in logprobs_remove_stopwords:\n",
    "        top_k_tokens_remove_stopwords.append(logprob.token)\n",
    "        top_k_logprobs_remove_stopwords.append(logprob.logprob)\n",
    "    raw_predictions_remove_stopwords.append({\"uid\": uid, \"top_k_tokens_remove_stopwords\": top_k_tokens_remove_stopwords, \"top_k_logprobs_remove_stopwords\": top_k_logprobs_remove_stopwords})\n",
    "\n",
    "# Write the results\n",
    "out_path = os.path.join('results', target_model)\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "# with open(os.path.join(out_path, f'raw_pred_{args.dataset_name}_{args.dataset_type}.json'), 'w') as fout:\n",
    "#     json.dump(raw_predictions, fout)\n",
    "\n",
    "with open(os.path.join(out_path, f'raw_pred_analogy_remove_stopwords.json'), 'w') as fout:\n",
    "    json.dump(raw_predictions_remove_stopwords, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "factual_knowledge_probing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
