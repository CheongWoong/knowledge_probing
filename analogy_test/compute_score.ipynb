{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sym_to_num = {'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = json.load(open('../data/analogy/MC_test.json', 'r'))\n",
    "gt[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_filtered = json.load(open('../data/analogy_filtered/MC_test.json', 'r'))\n",
    "gt_filtered_uids = [sample['uid'] for sample in gt_filtered]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['gpt-neo-125m', 'gpt-neo-1.3B', 'gpt-neo-2.7B', 'gpt-j-6b',\n",
    "                 'bert-base-uncased', 'bert-large-uncased',\n",
    "                 'roberta-base', 'roberta-large',\n",
    "                 'albert-base-v1', 'albert-large-v1', 'albert-xlarge-v1',\n",
    "                 'albert-base-v2', 'albert-large-v2', 'albert-xlarge-v2',\n",
    "                 'Meta-Llama-3-8B', 'Meta-Llama-3-8B-Instruct',\n",
    "                 ]\n",
    "\n",
    "for model_name in model_names:\n",
    "    try:\n",
    "        predictions = json.load(open(os.path.join('results', f'{model_name}.json'), 'r'))\n",
    "    except:\n",
    "        continue\n",
    "    pred_dict = {}\n",
    "    for pred in predictions:\n",
    "        pred_dict[pred['uid']] = pred\n",
    "\n",
    "    results = []\n",
    "    ranks, accs, tail_ranks, tail_accs = [], [], [], []\n",
    "\n",
    "    for sample in gt:\n",
    "        ppls = [pred_dict[sample['uid']+f'_{i}']['ppl'] for i in range(len(sample['choice']))]\n",
    "        tail_ppls = [pred_dict[sample['uid']+f'_{i}']['tail_ppl'] for i in range(len(sample['choice']))]\n",
    "        answer_idx = sym_to_num[sample['output']]\n",
    "        query = sample['query']\n",
    "        choice = sample['choice']\n",
    "\n",
    "        order = np.argsort(ppls).tolist()\n",
    "        rank = order.index(answer_idx)\n",
    "        accuracy = (rank == 0)*1\n",
    "        tail_order = np.argsort(tail_ppls).tolist()\n",
    "        tail_rank = tail_order.index(answer_idx)\n",
    "        tail_accuracy = (tail_rank == 0)*1\n",
    "\n",
    "        result = {'uid': sample['uid'], 'query': query, 'choice': choice, 'answer': answer_idx, 'order': order, 'rank': rank, 'accuracy': accuracy, 'ppl': ppls[answer_idx],\n",
    "                'tail_order': tail_order, 'tail_rank': tail_rank, 'tail_accuracy': tail_accuracy, 'tail_ppl': tail_ppls[answer_idx]}\n",
    "        results.append(result)\n",
    "\n",
    "        ranks.append(rank)\n",
    "        accs.append(accuracy)\n",
    "        tail_ranks.append(tail_rank)\n",
    "        tail_accs.append(tail_accuracy)\n",
    "\n",
    "    summary = {'rank': np.mean(ranks), 'accuracy': np.mean(accs),\n",
    "                    'tail_rank': np.mean(tail_ranks), 'tail_accuracy': np.mean(tail_accs)}\n",
    "\n",
    "    with open(f'results/{model_name}_pred.json', 'w') as fout:\n",
    "        json.dump(results, fout)\n",
    "    with open(f'results/{model_name}_summary.json', 'w') as fout:\n",
    "        json.dump(summary, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['gpt-neo-125m', 'gpt-neo-1.3B', 'gpt-neo-2.7B', 'gpt-j-6b',\n",
    "                 'bert-base-uncased', 'bert-large-uncased',\n",
    "                 'roberta-base', 'roberta-large',\n",
    "                 'albert-base-v1', 'albert-large-v1', 'albert-xlarge-v1',\n",
    "                 'albert-base-v2', 'albert-large-v2', 'albert-xlarge-v2',\n",
    "                 'Meta-Llama-3-8B', 'Meta-Llama-3-8B-Instruct',\n",
    "                 ]\n",
    "\n",
    "for model_name in model_names:\n",
    "    try:\n",
    "        predictions = json.load(open(os.path.join('results', f'{model_name}.json'), 'r'))\n",
    "    except:\n",
    "        continue\n",
    "    pred_dict = {}\n",
    "    for pred in predictions:\n",
    "        pred_dict[pred['uid']] = pred\n",
    "\n",
    "    results = []\n",
    "    ranks, accs, tail_ranks, tail_accs = [], [], [], []\n",
    "\n",
    "    for sample in gt_filtered:\n",
    "        ppls = [pred_dict[sample['uid']+f'_{i}']['ppl'] for i in range(len(sample['choice']))]\n",
    "        tail_ppls = [pred_dict[sample['uid']+f'_{i}']['tail_ppl'] for i in range(len(sample['choice']))]\n",
    "        answer_idx = sym_to_num[sample['output']]\n",
    "        query = sample['query']\n",
    "        choice = sample['choice']\n",
    "\n",
    "        order = np.argsort(ppls).tolist()\n",
    "        rank = order.index(answer_idx)\n",
    "        accuracy = (rank == 0)*1\n",
    "        tail_order = np.argsort(tail_ppls).tolist()\n",
    "        tail_rank = tail_order.index(answer_idx)\n",
    "        tail_accuracy = (tail_rank == 0)*1\n",
    "\n",
    "        result = {'uid': sample['uid'], 'query': query, 'choice': choice, 'answer': answer_idx, 'order': order, 'rank': rank, 'accuracy': accuracy, 'ppl': ppls[answer_idx],\n",
    "                'tail_order': tail_order, 'tail_rank': tail_rank, 'tail_accuracy': tail_accuracy, 'tail_ppl': tail_ppls[answer_idx]}\n",
    "        results.append(result)\n",
    "\n",
    "        ranks.append(rank)\n",
    "        accs.append(accuracy)\n",
    "        tail_ranks.append(tail_rank)\n",
    "        tail_accs.append(tail_accuracy)\n",
    "\n",
    "    summary = {'rank': np.mean(ranks), 'accuracy': np.mean(accs),\n",
    "                    'tail_rank': np.mean(tail_ranks), 'tail_accuracy': np.mean(tail_accs)}\n",
    "\n",
    "    with open(f'results/{model_name}_pred_filtered.json', 'w') as fout:\n",
    "        json.dump(results, fout)\n",
    "    with open(f'results/{model_name}_summary_filtered.json', 'w') as fout:\n",
    "        json.dump(summary, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['gpt-3.5-turbo-0125', 'gpt-4o-2024-08-06']\n",
    "\n",
    "for model_name in model_names:\n",
    "    predictions = json.load(open(os.path.join('results', model_name, 'raw_pred_analogy_remove_stopwords.json'), 'r'))\n",
    "    pred_dict = {}\n",
    "    for pred in predictions:\n",
    "        pred_dict[pred['uid']] = pred\n",
    "\n",
    "    results = []\n",
    "    accs = []\n",
    "\n",
    "    for sample in gt:\n",
    "        # ppls = [pred_dict[sample['uid']+f'_{i}']['ppl'] for i in range(5)]\n",
    "        # tail_ppls = [pred_dict[sample['uid']+f'_{i}']['tail_ppl'] for i in range(5)]\n",
    "        # print(sample['output'])\n",
    "        answer_idx = sym_to_num[sample['output']]\n",
    "        choice = sample['choice']\n",
    "        preds = pred_dict[sample['uid']]['top_k_tokens_remove_stopwords']\n",
    "        top_1_pred = preds[0].lower().strip()\n",
    "\n",
    "        accuracy = (sample['output'].lower().strip() in top_1_pred)*1\n",
    "\n",
    "        result = {'uid': sample['uid'], 'query': query, 'choice': choice, 'answer': answer_idx, 'accuracy': accuracy}\n",
    "        results.append(result)\n",
    "\n",
    "        accs.append(accuracy)\n",
    "\n",
    "    summary = {'accuracy': np.mean(accs)}\n",
    "\n",
    "    with open(f'results/{model_name}_pred.json', 'w') as fout:\n",
    "        json.dump(results, fout)\n",
    "    with open(f'results/{model_name}_summary.json', 'w') as fout:\n",
    "        json.dump(summary, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['gpt-3.5-turbo-0125', 'gpt-4o-2024-08-06']\n",
    "\n",
    "for model_name in model_names:\n",
    "    predictions = json.load(open(os.path.join('results', model_name, 'raw_pred_analogy_remove_stopwords.json'), 'r'))\n",
    "    pred_dict = {}\n",
    "    for pred in predictions:\n",
    "        pred_dict[pred['uid']] = pred\n",
    "\n",
    "    results = []\n",
    "    accs = []\n",
    "\n",
    "    for sample in gt_filtered:\n",
    "        # ppls = [pred_dict[sample['uid']+f'_{i}']['ppl'] for i in range(5)]\n",
    "        # tail_ppls = [pred_dict[sample['uid']+f'_{i}']['tail_ppl'] for i in range(5)]\n",
    "        # print(sample['output'])\n",
    "        answer_idx = sym_to_num[sample['output']]\n",
    "        choice = sample['choice']\n",
    "        preds = pred_dict[sample['uid']]['top_k_tokens_remove_stopwords']\n",
    "        top_1_pred = preds[0].lower().strip()\n",
    "\n",
    "        accuracy = (sample['output'].lower().strip() in top_1_pred)*1\n",
    "\n",
    "        result = {'uid': sample['uid'], 'query': query, 'choice': choice, 'answer': answer_idx, 'accuracy': accuracy}\n",
    "        results.append(result)\n",
    "\n",
    "        accs.append(accuracy)\n",
    "\n",
    "    summary = {'accuracy': np.mean(accs)}\n",
    "\n",
    "    with open(f'results/{model_name}_pred_filtered.json', 'w') as fout:\n",
    "        json.dump(results, fout)\n",
    "    with open(f'results/{model_name}_summary_filtered.json', 'w') as fout:\n",
    "        json.dump(summary, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "factual_knowledge_probing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
