{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import jsonlines\n",
    "from tqdm.auto import tqdm\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from cooccurrence_matrix import CooccurrenceMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pile_coo_matrix = CooccurrenceMatrix('pile')\n",
    "bert_coo_matrix = CooccurrenceMatrix('bert_pretraining_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "\n",
    "stopword_list = stopwords.words(\"english\")\n",
    "\n",
    "filter = {}\n",
    "for w in stopword_list:\n",
    "    filter[w] = w\n",
    "punctuations = {\n",
    "    \"?\": \"?\",\n",
    "    \":\": \":\",\n",
    "    \"!\": \"!\",\n",
    "    \".\": \".\",\n",
    "    \",\": \",\",\n",
    "    \";\": \";\"\n",
    "}\n",
    "filter.update(punctuations)\n",
    "def filtering(text):\n",
    "    if text in filter:\n",
    "        return True\n",
    "\n",
    "def text_normalization_without_lemmatization(text):\n",
    "    result = []\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    for token in tokens:\n",
    "        token_low = token.lower()\n",
    "        if filtering(token_low):\n",
    "            continue\n",
    "        result.append(token_low)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "# Scale factor for fonts\n",
    "scale_factor = 1.5\n",
    "\n",
    "# Update default font sizes\n",
    "plt.rcParams.update({\n",
    "    'font.size': 12 * scale_factor,\n",
    "    'axes.labelsize': 14 * scale_factor,  # x and y labels from plt.xlabel and plt.ylabel\n",
    "    'axes.titlesize': 16 * scale_factor,  # title from plt.title\n",
    "    'xtick.labelsize': 12 * scale_factor,  # x tick labels\n",
    "    'ytick.labelsize': 12 * scale_factor,  # y tick labels\n",
    "    'legend.fontsize': 12 * scale_factor,  # legend font size\n",
    "    'figure.titlesize': 18 * scale_factor  # suptitle\n",
    "})\n",
    "\n",
    "######################################################################\n",
    "\n",
    "dataset = 'bert'\n",
    "# dataset = 'pile'\n",
    "\n",
    "bin_edges = np.logspace(0, 7, 8)  # Create bin edges for log scale\n",
    "# bert_counts = [439, 764, 975, 651, 131, 4, 0]\n",
    "# pile_counts = [0, 9, 48, 474, 998, 984, 451]\n",
    "\n",
    "bert_counts = [9, 63, 291, 244, 49, 2, 0]\n",
    "pile_counts = [0, 0, 0, 4, 87, 337, 230]\n",
    "\n",
    "\n",
    "counts = {\n",
    "    'bert': bert_counts,\n",
    "    'pile': pile_counts\n",
    "}\n",
    "\n",
    "# Create the bar chart\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Create a bar chart\n",
    "# The 'align=edge' and the bin_edges[:-1] aligns bars starting from the left edge\n",
    "bars = ax.bar(np.power(10, np.arange(len(bin_edges[:-1]))+0.1), counts[dataset], width=np.diff(bin_edges)*0.8, align='edge', color='#17a2b8')\n",
    "\n",
    "# Set x-axis to logarithmic scales\n",
    "ax.set_xscale('log')\n",
    "plt.xticks(bin_edges, labels=[f'$10^{i}$' for i in range(len(bin_edges))])\n",
    "\n",
    "# remove minor ticks\n",
    "plt.tick_params(axis='x', which='minor', length=0)\n",
    "\n",
    "# Setting labels (adjust as needed)\n",
    "ax.set_xlabel('Joint frequency of the answer pair')\n",
    "ax.set_ylabel('Number of samples')\n",
    "\n",
    "# Annotate each bar with the count\n",
    "for bar, x_pos in zip(bars, np.power(10, np.arange(len(bin_edges[:-1]))+0.5)):\n",
    "    yval = bar.get_height()\n",
    "    ax.text(x_pos, yval, f'{yval:,}', va='bottom', ha='center', wrap=True)\n",
    "\n",
    "# Set the labels and title\n",
    "ax.get_yaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n",
    "\n",
    "plt.ylim(0, np.max(counts[dataset])*1.1)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()  # Adjust layout to fit all labels\n",
    "plt.savefig(f'results/number_of_samples_analogy_{dataset}.pdf', bbox_inches='tight', format='pdf')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_filtered = json.load(open('../data/analogy_filtered/MC_test.json', 'r'))\n",
    "gt_filtered_uids = [sample['uid'] for sample in gt_filtered]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [0, 10, 100, 1000, 10000, 100000, 1000000]\n",
    "# bins = [0, 10, 100, 1000, 10000, 100000]\n",
    "# bins = [0, 10, 100, 1000]\n",
    "\n",
    "def frequency_to_section(value):\n",
    "    return np.digitize(value, bins)\n",
    "\n",
    "def frequency_section_to_string(section):\n",
    "    return f'{section}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_dict = {\n",
    "    'bert-base-uncased': 'BERT$_{base}$',\n",
    "    'bert-large-uncased': 'BERT$_{large}$',\n",
    "    'gpt-j-6b': 'GPT-J 6B',\n",
    "    'Meta-Llama-3-8B': 'Llama-3 8B',\n",
    "}\n",
    "\n",
    "colors = {\n",
    "    'bert-base-uncased': 'tab:blue',\n",
    "    'bert-large-uncased': 'tab:green',\n",
    "    'gpt-j-6b': 'tab:red',\n",
    "    'Meta-Llama-3-8B': 'tab:orange',\n",
    "}\n",
    "markers = {\n",
    "    'bert-base-uncased': 'o',\n",
    "    'bert-large-uncased': '^',\n",
    "    'gpt-j-6b': 's',\n",
    "    'Meta-Llama-3-8B': 'D',\n",
    "}\n",
    "\n",
    "# Scale factor for fonts\n",
    "scale_factor = 1.5\n",
    "\n",
    "# Update default font sizes\n",
    "plt.rcParams.update({\n",
    "    'font.size': 12 * scale_factor,\n",
    "    'axes.labelsize': 14 * scale_factor,  # x and y labels from plt.xlabel and plt.ylabel\n",
    "    'axes.titlesize': 16 * scale_factor,  # title from plt.title\n",
    "    'xtick.labelsize': 12 * scale_factor,  # x tick labels\n",
    "    'ytick.labelsize': 12 * scale_factor,  # y tick labels\n",
    "    'legend.fontsize': 12 * scale_factor,  # legend font size\n",
    "    'figure.titlesize': 18 * scale_factor  # suptitle\n",
    "})\n",
    "\n",
    "# Fixed x-axis values - the positions where the x-tick labels will be placed\n",
    "x_tick_labels = [1, 10, 100, 1000, 10000, 100000, 1000000, 10000000]\n",
    "# x_tick_labels = [1, 10, 100, 1000, 10000, 100000, 1000000]\n",
    "# x_tick_labels = [1, 10, 100, 1000, 10000]\n",
    "# Calculate midpoints for plotting the data points\n",
    "x_values = np.sqrt(np.array(x_tick_labels[:-1]) * np.array(x_tick_labels[1:]))\n",
    "\n",
    "# Create a figure and a set of subplots\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for model_name in model_name_dict.keys():\n",
    "    print('='*30)\n",
    "    print('='*30)\n",
    "    print('Model:', model_name)\n",
    "\n",
    "    try:\n",
    "        data = json.load(open(f'results/{model_name}_pred.json', 'r'))\n",
    "    except:\n",
    "        raise Exception\n",
    "        # continue\n",
    "\n",
    "    if 'gpt' in model_name or 'Llama' in model_name:\n",
    "        coo_matrix = pile_coo_matrix\n",
    "        num_total_samples = 254188957\n",
    "    else:\n",
    "        coo_matrix = bert_coo_matrix\n",
    "        num_total_samples = 158887337\n",
    "    # coo_matrix = bert_coo_matrix\n",
    "    # num_total_samples = 158887337\n",
    "\n",
    "    openai_api = True if 'gpt-3.5-turbo' in model_name or 'gpt-4o' in model_name else False\n",
    "\n",
    "    results_rank, results_acc, results_tail_rank, results_tail_acc = defaultdict(list), defaultdict(list), defaultdict(list), defaultdict(list)\n",
    "    results_ppl, results_tail_ppl = defaultdict(list), defaultdict(list)\n",
    "\n",
    "    for pred in tqdm(data):\n",
    "        uid = pred['uid']\n",
    "        if uid not in gt_filtered_uids:\n",
    "            continue\n",
    "\n",
    "        flag = True\n",
    "        choices = pred['choice']\n",
    "        freqs = []\n",
    "        for c_idx, choice in enumerate(choices):\n",
    "            subj, obj = choice\n",
    "            subj = ' '.join(text_normalization_without_lemmatization(subj))\n",
    "            obj = ' '.join(text_normalization_without_lemmatization(obj))\n",
    "\n",
    "            subj_count = coo_matrix.count(subj)\n",
    "            obj_count = coo_matrix.count(obj)\n",
    "            subj_obj_count = coo_matrix.coo_count(subj, obj)\n",
    "            \n",
    "            if subj_obj_count < 0:\n",
    "                flag = False\n",
    "                break\n",
    "\n",
    "            freq = subj_obj_count\n",
    "            \n",
    "            # try:\n",
    "            #     freq = 1 / (freq/subj_count)\n",
    "            # except:\n",
    "            #     freq = 0\n",
    "\n",
    "            freqs.append(freq)\n",
    "        \n",
    "        if not flag:\n",
    "            continue\n",
    "\n",
    "        ans_freq = freqs[pred['answer']]\n",
    "        freq_order = np.argsort(freqs).tolist()\n",
    "        freq_rank = freq_order.index(pred['answer'])\n",
    "\n",
    "        section = frequency_to_section(ans_freq)\n",
    "\n",
    "        results_rank[section].append(1 + pred['rank'])\n",
    "        results_acc[section].append(pred['accuracy'])\n",
    "        results_ppl[section].append(pred['ppl'])\n",
    "        results_tail_rank[section].append(1 + pred['tail_rank'])\n",
    "        results_tail_acc[section].append(pred['tail_accuracy'])\n",
    "        results_tail_ppl[section].append(pred['tail_ppl'])\n",
    "\n",
    "    num_samples = {}\n",
    "    sections = range(1, len(bins)+1)\n",
    "    for section in sections:\n",
    "        num_samples[section] = len(results_rank[section])\n",
    "\n",
    "        if section in results_rank:\n",
    "            results_rank[section] = np.mean(results_rank[section]), np.std(results_rank[section])\n",
    "            results_acc[section] = np.mean(results_acc[section]), np.std(results_acc[section])\n",
    "            results_ppl[section] = np.mean(results_ppl[section]), np.std(results_ppl[section])\n",
    "            results_tail_rank[section] = np.mean(results_tail_rank[section]), np.std(results_tail_rank[section])\n",
    "            results_tail_acc[section] = np.mean(results_tail_acc[section]), np.std(results_tail_acc[section])\n",
    "            results_tail_ppl[section] = np.mean(results_tail_ppl[section]), np.std(results_tail_ppl[section])\n",
    "\n",
    "    result = {}\n",
    "    for section in sections:\n",
    "        if section in results_rank:\n",
    "            result[f'result_rank_section_{frequency_section_to_string(section)}'] = f'%.2f +- %.2f' % results_rank[section]\n",
    "    for section in sections:\n",
    "        if section in results_acc:\n",
    "            result[f'result_accuracy_section_{frequency_section_to_string(section)}'] = f'%.2f +- %.2f' % results_acc[section]\n",
    "    for section in sections:\n",
    "        if section in results_ppl:\n",
    "            result[f'result_ppl_section_{frequency_section_to_string(section)}'] = f'%.2f +- %.2f' % results_ppl[section]\n",
    "    for section in sections:\n",
    "        if section in results_tail_rank:\n",
    "            result[f'result_tail_rank_section_{frequency_section_to_string(section)}'] = f'%.2f +- %.2f' % results_tail_rank[section]\n",
    "    for section in sections:\n",
    "        if section in results_tail_acc:\n",
    "            result[f'result_tail_accuracy_section_{frequency_section_to_string(section)}'] = f'%.2f +- %.2f' % results_tail_acc[section]\n",
    "    for section in sections:\n",
    "        if section in results_tail_ppl:\n",
    "            result[f'result_tail_ppl_section_{frequency_section_to_string(section)}'] = f'%.2f +- %.2f' % results_tail_ppl[section]\n",
    "            \n",
    "    print(num_samples)\n",
    "\n",
    "    hits_100_mean = [results_acc[section][0] for section in sections]\n",
    "    print(hits_100_mean)\n",
    "    hits_100_std = [results_rank[section][1] for section in sections]\n",
    "    # Plotting line plots for Hits@100\n",
    "    ax1.plot(x_values, hits_100_mean, marker=markers[model_name], color=colors[model_name], linestyle='-', label=model_name_dict[model_name])\n",
    "    \n",
    "# Set x-axis to a logarithmic scale\n",
    "plt.xscale('log')\n",
    "plt.xticks(x_tick_labels, labels=[f'$10^{i}$' for i in range(len(x_tick_labels))])\n",
    "\n",
    "# remove minor ticks\n",
    "plt.tick_params(axis='x', which='minor', length=0)\n",
    "\n",
    "# Setting the x-axis label\n",
    "plt.xlabel('Joint frequency of answer pair')\n",
    "# Setting the y-axis label for the first y-axis\n",
    "ax1.set_ylabel('Accuracy', color='black')\n",
    "# ax1.set_ylabel('Rank', color='black')\n",
    "# Set the limits for the y-axis if necessary\n",
    "ax1.set_ylim(0, 1)\n",
    "\n",
    "# Adding a legend for the line plots\n",
    "ax1.legend()\n",
    "\n",
    "# Show the plot\n",
    "# plt.title('Model Performance Comparison')\n",
    "filename = f'results/analogy_test_zeroshot_accuracy_against_jointprob.pdf'\n",
    "plt.tight_layout()  # Adjust layout to fit all labels\n",
    "plt.savefig(filename, format='pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [0, 10, 100, 1000, 10000, 100000, 1000000]\n",
    "# bins = [0, 10, 100, 1000, 10000, 100000]\n",
    "# bins = [0, 10, 100, 1000]\n",
    "\n",
    "def frequency_to_section(value):\n",
    "    return np.digitize(value, bins)\n",
    "\n",
    "def frequency_section_to_string(section):\n",
    "    return f'{section}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_dict = {\n",
    "    # 'bert-base-uncased': 'BERT$_{base}$',\n",
    "    # 'bert-large-uncased': 'BERT$_{large}$',\n",
    "    # 'albert-base-v1': 'ALBERT1$_{base}$',\n",
    "    # 'albert-large-v1': 'ALBERT1$_{large}$',\n",
    "    # 'albert-xlarge-v1': 'ALBERT1$_{xlarge}$',\n",
    "    # 'albert-base-v2': 'ALBERT2$_{base}$',\n",
    "    # 'albert-large-v2': 'ALBERT2$_{large}$',\n",
    "    # 'albert-xlarge-v2': 'ALBERT2$_{xlarge}$',\n",
    "    # 'roberta-base': 'RoBERTa$_{base}$',\n",
    "    # 'roberta-large': 'RoBERTa$_{large}$',\n",
    "    # 'gpt-neo-125m': 'GPT-Neo 125M',\n",
    "    # 'gpt-neo-1.3B': 'GPT-Neo 1.3B',\n",
    "    # 'gpt-neo-2.7B': 'GPT-Neo 2.7B',\n",
    "    # 'gpt-j-6b': 'GPT-J 6B',\n",
    "    'gpt-3.5-turbo-0125': 'ChatGPT-3.5',\n",
    "    'gpt-4o-2024-08-06': 'ChatGPT-4o'\n",
    "}\n",
    "\n",
    "colors = {\n",
    "    'gpt-3.5-turbo-0125': 'tab:blue',\n",
    "    'gpt-4o-2024-08-06': 'tab:green',\n",
    "}\n",
    "markers = {\n",
    "    'gpt-3.5-turbo-0125': 'o',\n",
    "    'gpt-4o-2024-08-06': '^',\n",
    "}\n",
    "\n",
    "# Scale factor for fonts\n",
    "scale_factor = 1.5\n",
    "\n",
    "# Update default font sizes\n",
    "plt.rcParams.update({\n",
    "    'font.size': 12 * scale_factor,\n",
    "    'axes.labelsize': 14 * scale_factor,  # x and y labels from plt.xlabel and plt.ylabel\n",
    "    'axes.titlesize': 16 * scale_factor,  # title from plt.title\n",
    "    'xtick.labelsize': 12 * scale_factor,  # x tick labels\n",
    "    'ytick.labelsize': 12 * scale_factor,  # y tick labels\n",
    "    'legend.fontsize': 12 * scale_factor,  # legend font size\n",
    "    'figure.titlesize': 18 * scale_factor  # suptitle\n",
    "})\n",
    "\n",
    "# Fixed x-axis values - the positions where the x-tick labels will be placed\n",
    "x_tick_labels = [1, 10, 100, 1000, 10000, 100000, 1000000, 10000000]\n",
    "# x_tick_labels = [1, 10, 100, 1000, 10000, 100000, 1000000]\n",
    "# x_tick_labels = [1, 10, 100, 1000, 10000]\n",
    "# Calculate midpoints for plotting the data points\n",
    "x_values = np.sqrt(np.array(x_tick_labels[:-1]) * np.array(x_tick_labels[1:]))\n",
    "\n",
    "# Create a figure and a set of subplots\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for model_name in model_name_dict.keys():\n",
    "    print('='*30)\n",
    "    print('='*30)\n",
    "    print('Model:', model_name)\n",
    "\n",
    "    try:\n",
    "        data = json.load(open(f'results/{model_name}_pred.json', 'r'))\n",
    "    except:\n",
    "        raise Exception\n",
    "        # continue\n",
    "\n",
    "    if 'gpt' in model_name or 'Llama' in model_name:\n",
    "        coo_matrix = pile_coo_matrix\n",
    "        num_total_samples = 254188957\n",
    "    else:\n",
    "        coo_matrix = bert_coo_matrix\n",
    "        num_total_samples = 158887337\n",
    "    # coo_matrix = bert_coo_matrix\n",
    "    # num_total_samples = 158887337\n",
    "\n",
    "    openai_api = True if 'gpt-3.5-turbo' in model_name or 'gpt-4o' in model_name else False\n",
    "\n",
    "    results_rank, results_acc, results_tail_rank, results_tail_acc = defaultdict(list), defaultdict(list), defaultdict(list), defaultdict(list)\n",
    "\n",
    "    for pred in tqdm(data):\n",
    "        uid = pred['uid']\n",
    "        if uid not in gt_filtered_uids:\n",
    "            continue\n",
    "\n",
    "        flag = True\n",
    "        choices = pred['choice']\n",
    "        freqs = []\n",
    "        for c_idx, choice in enumerate(choices):\n",
    "            subj, obj = choice\n",
    "            subj = ' '.join(text_normalization_without_lemmatization(subj))\n",
    "            obj = ' '.join(text_normalization_without_lemmatization(obj))\n",
    "\n",
    "            subj_count = coo_matrix.count(subj)\n",
    "            obj_count = coo_matrix.count(obj)\n",
    "            subj_obj_count = coo_matrix.coo_count(subj, obj)\n",
    "            \n",
    "            if subj_obj_count < 0:\n",
    "                flag = False\n",
    "                break\n",
    "\n",
    "            freq = subj_obj_count\n",
    "            freqs.append(freq)\n",
    "        \n",
    "        if not flag:\n",
    "            continue\n",
    "\n",
    "        ans_freq = freqs[pred['answer']]\n",
    "        freq_order = np.argsort(freqs).tolist()\n",
    "        freq_rank = freq_order.index(pred['answer'])\n",
    "\n",
    "        section = frequency_to_section(ans_freq)\n",
    "\n",
    "        # results_rank[section].append(1 + pred['rank'])\n",
    "        results_acc[section].append(pred['accuracy'])\n",
    "        # results_tail_rank[section].append(1 + pred['tail_rank'])\n",
    "        # results_tail_acc[section].append(pred['tail_accuracy'])\n",
    "\n",
    "    num_samples = {}\n",
    "    sections = range(1, len(bins)+1)\n",
    "    for section in sections:\n",
    "        num_samples[section] = len(results_acc[section])\n",
    "\n",
    "        if section in results_acc:\n",
    "            # results_rank[section] = np.mean(results_rank[section]), np.std(results_rank[section])\n",
    "            results_acc[section] = np.mean(results_acc[section]), np.std(results_acc[section])\n",
    "            # results_tail_rank[section] = np.mean(results_tail_rank[section]), np.std(results_tail_rank[section])\n",
    "            # results_tail_acc[section] = np.mean(results_tail_acc[section]), np.std(results_tail_acc[section])\n",
    "\n",
    "    result = {}\n",
    "    # for section in sections:\n",
    "    #     if section in results_rank:\n",
    "    #         result[f'result_rank_section_{frequency_section_to_string(section)}'] = f'%.2f +- %.2f' % results_rank[section]\n",
    "    for section in sections:\n",
    "        if section in results_acc:\n",
    "            result[f'result_accuracy_section_{frequency_section_to_string(section)}'] = f'%.2f +- %.2f' % results_acc[section]\n",
    "    # for section in sections:\n",
    "    #     if section in results_tail_rank:\n",
    "    #         result[f'result_tail_rank_section_{frequency_section_to_string(section)}'] = f'%.2f +- %.2f' % results_tail_rank[section]\n",
    "    # for section in sections:\n",
    "    #     if section in results_tail_acc:\n",
    "    #         result[f'result_tail_accuracy_section_{frequency_section_to_string(section)}'] = f'%.2f +- %.2f' % results_tail_acc[section]\n",
    "            \n",
    "    print(num_samples)\n",
    "\n",
    "    hits_100_mean = [results_acc[section][0] for section in sections]\n",
    "    print(hits_100_mean)\n",
    "    hits_100_std = [results_acc[section][1] for section in sections]\n",
    "    # Plotting line plots for Hits@100\n",
    "    ax1.plot(x_values, hits_100_mean, marker=markers[model_name], color=colors[model_name], linestyle='-', label=model_name_dict[model_name])\n",
    "    \n",
    "# Set x-axis to a logarithmic scale\n",
    "plt.xscale('log')\n",
    "plt.xticks(x_tick_labels, labels=[f'$10^{i}$' for i in range(len(x_tick_labels))])\n",
    "\n",
    "# remove minor ticks\n",
    "plt.tick_params(axis='x', which='minor', length=0)\n",
    "\n",
    "# Setting the x-axis label\n",
    "plt.xlabel('Joint frequency of answer pair')\n",
    "# Setting the y-axis label for the first y-axis\n",
    "ax1.set_ylabel('Accuracy', color='black')\n",
    "# ax1.set_ylabel('Rank', color='black')\n",
    "# Set the limits for the y-axis if necessary\n",
    "ax1.set_ylim(0, 1)\n",
    "\n",
    "# Adding a legend for the line plots\n",
    "ax1.legend()\n",
    "\n",
    "# Show the plot\n",
    "# plt.title('Model Performance Comparison')\n",
    "filename = f'results/analogy_test_chatgpt_zeroshot_accuracy_against_jointprob.pdf'\n",
    "plt.tight_layout()  # Adjust layout to fit all labels\n",
    "plt.savefig(filename, format='pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_dict = {\n",
    "    # 'bert-base-uncased': 'BERT$_{base}$',\n",
    "    # 'bert-large-uncased': 'BERT$_{large}$',\n",
    "    # 'albert-base-v1': 'ALBERT1$_{base}$',\n",
    "    # 'albert-large-v1': 'ALBERT1$_{large}$',\n",
    "    # 'albert-xlarge-v1': 'ALBERT1$_{xlarge}$',\n",
    "    # 'albert-base-v2': 'ALBERT2$_{base}$',\n",
    "    # 'albert-large-v2': 'ALBERT2$_{large}$',\n",
    "    # 'albert-xlarge-v2': 'ALBERT2$_{xlarge}$',\n",
    "    # 'roberta-base': 'RoBERTa$_{base}$',\n",
    "    # 'roberta-large': 'RoBERTa$_{large}$',\n",
    "    'gpt-neo-125m': 'GPT-Neo 125M',\n",
    "    'gpt-neo-1.3B': 'GPT-Neo 1.3B',\n",
    "    'gpt-neo-2.7B': 'GPT-Neo 2.7B',\n",
    "    'Meta-Llama-3-8B-Instruct': 'Llama-3 8B Instruct',\n",
    "    # 'gpt-j-6b': 'GPT-J 6B',\n",
    "    # 'gpt-3.5-turbo-0125': 'ChatGPT-3.5',\n",
    "    # 'gpt-4-0125-preview': 'ChatGPT-4'\n",
    "}\n",
    "\n",
    "colors = {\n",
    "    'gpt-neo-125m': 'tab:blue',\n",
    "    'gpt-neo-1.3B': 'tab:green',\n",
    "    'gpt-neo-2.7B': 'tab:red',\n",
    "    # 'gpt-j-6b': 'tab:orange',\n",
    "    'Meta-Llama-3-8B-Instruct': 'tab:orange',\n",
    "}\n",
    "markers = {\n",
    "    'gpt-neo-125m': 'o',\n",
    "    'gpt-neo-1.3B': '^',\n",
    "    'gpt-neo-2.7B': 's',\n",
    "    # 'gpt-j-6b': 'D',\n",
    "    'Meta-Llama-3-8B-Instruct': 'D',\n",
    "}\n",
    "\n",
    "# Scale factor for fonts\n",
    "scale_factor = 1.5\n",
    "\n",
    "# Update default font sizes\n",
    "plt.rcParams.update({\n",
    "    'font.size': 12 * scale_factor,\n",
    "    'axes.labelsize': 14 * scale_factor,  # x and y labels from plt.xlabel and plt.ylabel\n",
    "    'axes.titlesize': 16 * scale_factor,  # title from plt.title\n",
    "    'xtick.labelsize': 12 * scale_factor,  # x tick labels\n",
    "    'ytick.labelsize': 12 * scale_factor,  # y tick labels\n",
    "    'legend.fontsize': 12 * scale_factor,  # legend font size\n",
    "    'figure.titlesize': 18 * scale_factor  # suptitle\n",
    "})\n",
    "\n",
    "# Fixed x-axis values - the positions where the x-tick labels will be placed\n",
    "x_tick_labels = [1, 10, 100, 1000, 10000, 100000, 1000000, 10000000]\n",
    "# x_tick_labels = [1, 10, 100, 1000, 10000, 100000, 1000000]\n",
    "# x_tick_labels = [1, 10, 100, 1000, 10000]\n",
    "# Calculate midpoints for plotting the data points\n",
    "x_values = np.sqrt(np.array(x_tick_labels[:-1]) * np.array(x_tick_labels[1:]))\n",
    "\n",
    "# Create a figure and a set of subplots\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for model_name in model_name_dict.keys():\n",
    "    print('='*30)\n",
    "    print('='*30)\n",
    "    print('Model:', model_name)\n",
    "\n",
    "    try:\n",
    "        data = json.load(open(f'results/{model_name}_pred.json', 'r'))\n",
    "    except:\n",
    "        raise Exception\n",
    "        # continue\n",
    "\n",
    "    if 'gpt' in model_name or 'Llama' in model_name:\n",
    "        coo_matrix = pile_coo_matrix\n",
    "        num_total_samples = 254188957\n",
    "    else:\n",
    "        coo_matrix = bert_coo_matrix\n",
    "        num_total_samples = 158887337\n",
    "    # coo_matrix = bert_coo_matrix\n",
    "    # num_total_samples = 158887337\n",
    "\n",
    "    openai_api = True if 'gpt-3.5-turbo' in model_name or 'gpt-4o' in model_name else False\n",
    "\n",
    "    results_rank, results_acc, results_tail_rank, results_tail_acc = defaultdict(list), defaultdict(list), defaultdict(list), defaultdict(list)\n",
    "\n",
    "    for pred in tqdm(data):\n",
    "        uid = pred['uid']\n",
    "        if uid not in gt_filtered_uids:\n",
    "            continue\n",
    "        \n",
    "        flag = True\n",
    "        choices = pred['choice']\n",
    "        freqs = []\n",
    "        for c_idx, choice in enumerate(choices):\n",
    "            subj, obj = choice\n",
    "            subj = ' '.join(text_normalization_without_lemmatization(subj))\n",
    "            obj = ' '.join(text_normalization_without_lemmatization(obj))\n",
    "\n",
    "            subj_count = coo_matrix.count(subj)\n",
    "            obj_count = coo_matrix.count(obj)\n",
    "            subj_obj_count = coo_matrix.coo_count(subj, obj)\n",
    "            \n",
    "            if subj_obj_count < 0:\n",
    "                flag = False\n",
    "                break\n",
    "\n",
    "            freq = subj_obj_count\n",
    "            freqs.append(freq)\n",
    "        \n",
    "        if not flag:\n",
    "            continue\n",
    "\n",
    "        ans_freq = freqs[pred['answer']]\n",
    "        freq_order = np.argsort(freqs).tolist()\n",
    "        freq_rank = freq_order.index(pred['answer'])\n",
    "\n",
    "        section = frequency_to_section(ans_freq)\n",
    "\n",
    "        results_rank[section].append(1 + pred['rank'])\n",
    "        results_acc[section].append(pred['accuracy'])\n",
    "        results_tail_rank[section].append(1 + pred['tail_rank'])\n",
    "        results_tail_acc[section].append(pred['tail_accuracy'])\n",
    "\n",
    "    num_samples = {}\n",
    "    sections = range(1, len(bins)+1)\n",
    "    for section in sections:\n",
    "        num_samples[section] = len(results_rank[section])\n",
    "\n",
    "        if section in results_rank:\n",
    "            results_rank[section] = np.mean(results_rank[section]), np.std(results_rank[section])\n",
    "            results_acc[section] = np.mean(results_acc[section]), np.std(results_acc[section])\n",
    "            results_tail_rank[section] = np.mean(results_tail_rank[section]), np.std(results_tail_rank[section])\n",
    "            results_tail_acc[section] = np.mean(results_tail_acc[section]), np.std(results_tail_acc[section])\n",
    "\n",
    "    result = {}\n",
    "    for section in sections:\n",
    "        if section in results_rank:\n",
    "            result[f'result_rank_section_{frequency_section_to_string(section)}'] = f'%.2f +- %.2f' % results_rank[section]\n",
    "    for section in sections:\n",
    "        if section in results_acc:\n",
    "            result[f'result_accuracy_section_{frequency_section_to_string(section)}'] = f'%.2f +- %.2f' % results_acc[section]\n",
    "    for section in sections:\n",
    "        if section in results_tail_rank:\n",
    "            result[f'result_tail_rank_section_{frequency_section_to_string(section)}'] = f'%.2f +- %.2f' % results_tail_rank[section]\n",
    "    for section in sections:\n",
    "        if section in results_tail_acc:\n",
    "            result[f'result_tail_accuracy_section_{frequency_section_to_string(section)}'] = f'%.2f +- %.2f' % results_tail_acc[section]\n",
    "            \n",
    "    print(num_samples)\n",
    "\n",
    "    hits_100_mean = [results_acc[section][0] for section in sections]\n",
    "    hits_100_std = [results_rank[section][1] for section in sections]\n",
    "    # Plotting line plots for Hits@100\n",
    "    ax1.plot(x_values, hits_100_mean, marker=markers[model_name], color=colors[model_name], linestyle='-', label=model_name_dict[model_name])\n",
    "    \n",
    "# Set x-axis to a logarithmic scale\n",
    "plt.xscale('log')\n",
    "plt.xticks(x_tick_labels, labels=[f'$10^{i}$' for i in range(len(x_tick_labels))])\n",
    "\n",
    "# remove minor ticks\n",
    "plt.tick_params(axis='x', which='minor', length=0)\n",
    "\n",
    "# Setting the x-axis label\n",
    "plt.xlabel('Joint frequency of answer pair')\n",
    "# Setting the y-axis label for the first y-axis\n",
    "ax1.set_ylabel('Accuracy', color='black')\n",
    "# ax1.set_ylabel('Rank', color='black')\n",
    "# Set the limits for the y-axis if necessary\n",
    "ax1.set_ylim(0, 1)\n",
    "\n",
    "# Adding a legend for the line plots\n",
    "ax1.legend()\n",
    "\n",
    "# Show the plot\n",
    "# plt.title('Model Performance Comparison')\n",
    "filename = f'results/analogy_test_glms_zeroshot_accuracy_against_jointprob.pdf'\n",
    "plt.tight_layout()  # Adjust layout to fit all labels\n",
    "plt.savefig(filename, format='pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_dict = {\n",
    "    # 'bert-base-uncased': 'BERT$_{base}$',\n",
    "    # 'bert-large-uncased': 'BERT$_{large}$',\n",
    "    # 'albert-base-v1': 'ALBERT1$_{base}$',\n",
    "    # 'albert-large-v1': 'ALBERT1$_{large}$',\n",
    "    # 'albert-xlarge-v1': 'ALBERT1$_{xlarge}$',\n",
    "    # 'albert-base-v2': 'ALBERT2$_{base}$',\n",
    "    # 'albert-large-v2': 'ALBERT2$_{large}$',\n",
    "    # 'albert-xlarge-v2': 'ALBERT2$_{xlarge}$',\n",
    "    'roberta-base': 'RoBERTa$_{base}$',\n",
    "    'roberta-large': 'RoBERTa$_{large}$',\n",
    "    # 'gpt-neo-125m': 'GPT-Neo 125M',\n",
    "    # 'gpt-neo-1.3B': 'GPT-Neo 1.3B',\n",
    "    # 'gpt-neo-2.7B': 'GPT-Neo 2.7B',\n",
    "    # 'gpt-j-6b': 'GPT-J 6B',\n",
    "    # 'gpt-3.5-turbo-0125': 'ChatGPT-3.5',\n",
    "    # 'gpt-4-0125-preview': 'ChatGPT-4'\n",
    "}\n",
    "\n",
    "colors = {\n",
    "    'bert-base-uncased': 'tab:blue',\n",
    "    'bert-large-uncased': 'tab:green',\n",
    "    'roberta-base': 'tab:red',\n",
    "    'roberta-large': 'tab:orange',\n",
    "}\n",
    "markers = {\n",
    "    'bert-base-uncased': 'o',\n",
    "    'bert-large-uncased': '^',\n",
    "    'roberta-base': 's',\n",
    "    'roberta-large': 'D',\n",
    "}\n",
    "\n",
    "# Scale factor for fonts\n",
    "scale_factor = 1.5\n",
    "\n",
    "# Update default font sizes\n",
    "plt.rcParams.update({\n",
    "    'font.size': 12 * scale_factor,\n",
    "    'axes.labelsize': 14 * scale_factor,  # x and y labels from plt.xlabel and plt.ylabel\n",
    "    'axes.titlesize': 16 * scale_factor,  # title from plt.title\n",
    "    'xtick.labelsize': 12 * scale_factor,  # x tick labels\n",
    "    'ytick.labelsize': 12 * scale_factor,  # y tick labels\n",
    "    'legend.fontsize': 12 * scale_factor,  # legend font size\n",
    "    'figure.titlesize': 18 * scale_factor  # suptitle\n",
    "})\n",
    "\n",
    "# Fixed x-axis values - the positions where the x-tick labels will be placed\n",
    "x_tick_labels = [1, 10, 100, 1000, 10000, 100000, 1000000, 10000000]\n",
    "# x_tick_labels = [1, 10, 100, 1000, 10000, 100000, 1000000]\n",
    "# x_tick_labels = [1, 10, 100, 1000, 10000]\n",
    "# Calculate midpoints for plotting the data points\n",
    "x_values = np.sqrt(np.array(x_tick_labels[:-1]) * np.array(x_tick_labels[1:]))\n",
    "\n",
    "# Create a figure and a set of subplots\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for model_name in model_name_dict.keys():\n",
    "    print('='*30)\n",
    "    print('='*30)\n",
    "    print('Model:', model_name)\n",
    "\n",
    "    try:\n",
    "        data = json.load(open(f'results/{model_name}_pred.json', 'r'))\n",
    "    except:\n",
    "        raise Exception\n",
    "        # continue\n",
    "\n",
    "    if 'gpt' in model_name or 'Llama' in model_name:\n",
    "        coo_matrix = pile_coo_matrix\n",
    "        num_total_samples = 254188957\n",
    "    else:\n",
    "        coo_matrix = bert_coo_matrix\n",
    "        num_total_samples = 158887337\n",
    "    # coo_matrix = bert_coo_matrix\n",
    "    # num_total_samples = 158887337\n",
    "\n",
    "    openai_api = True if 'gpt-3.5-turbo' in model_name or 'gpt-4o' in model_name else False\n",
    "\n",
    "    results_rank, results_acc, results_tail_rank, results_tail_acc = defaultdict(list), defaultdict(list), defaultdict(list), defaultdict(list)\n",
    "\n",
    "    for pred in tqdm(data):\n",
    "        uid = pred['uid']\n",
    "        if uid not in gt_filtered_uids:\n",
    "            continue\n",
    "\n",
    "        flag = True\n",
    "        choices = pred['choice']\n",
    "        freqs = []\n",
    "        for c_idx, choice in enumerate(choices):\n",
    "            subj, obj = choice\n",
    "            subj = ' '.join(text_normalization_without_lemmatization(subj))\n",
    "            obj = ' '.join(text_normalization_without_lemmatization(obj))\n",
    "\n",
    "            subj_count = coo_matrix.count(subj)\n",
    "            obj_count = coo_matrix.count(obj)\n",
    "            subj_obj_count = coo_matrix.coo_count(subj, obj)\n",
    "            \n",
    "            if subj_obj_count < 0:\n",
    "                flag = False\n",
    "                break\n",
    "\n",
    "            freq = subj_obj_count\n",
    "            freqs.append(freq)\n",
    "        \n",
    "        if not flag:\n",
    "            continue\n",
    "\n",
    "        ans_freq = freqs[pred['answer']]\n",
    "        freq_order = np.argsort(freqs).tolist()\n",
    "        freq_rank = freq_order.index(pred['answer'])\n",
    "\n",
    "        section = frequency_to_section(ans_freq)\n",
    "\n",
    "        results_rank[section].append(1 + pred['rank'])\n",
    "        results_acc[section].append(pred['accuracy'])\n",
    "        results_tail_rank[section].append(1 + pred['tail_rank'])\n",
    "        results_tail_acc[section].append(pred['tail_accuracy'])\n",
    "\n",
    "    num_samples = {}\n",
    "    sections = range(1, len(bins)+1)\n",
    "    for section in sections:\n",
    "        num_samples[section] = len(results_rank[section])\n",
    "\n",
    "        if section in results_rank:\n",
    "            results_rank[section] = np.mean(results_rank[section]), np.std(results_rank[section])\n",
    "            results_acc[section] = np.mean(results_acc[section]), np.std(results_acc[section])\n",
    "            results_tail_rank[section] = np.mean(results_tail_rank[section]), np.std(results_tail_rank[section])\n",
    "            results_tail_acc[section] = np.mean(results_tail_acc[section]), np.std(results_tail_acc[section])\n",
    "\n",
    "    result = {}\n",
    "    for section in sections:\n",
    "        if section in results_rank:\n",
    "            result[f'result_rank_section_{frequency_section_to_string(section)}'] = f'%.2f +- %.2f' % results_rank[section]\n",
    "    for section in sections:\n",
    "        if section in results_acc:\n",
    "            result[f'result_accuracy_section_{frequency_section_to_string(section)}'] = f'%.2f +- %.2f' % results_acc[section]\n",
    "    for section in sections:\n",
    "        if section in results_tail_rank:\n",
    "            result[f'result_tail_rank_section_{frequency_section_to_string(section)}'] = f'%.2f +- %.2f' % results_tail_rank[section]\n",
    "    for section in sections:\n",
    "        if section in results_tail_acc:\n",
    "            result[f'result_tail_accuracy_section_{frequency_section_to_string(section)}'] = f'%.2f +- %.2f' % results_tail_acc[section]\n",
    "            \n",
    "    print(num_samples)\n",
    "\n",
    "    hits_100_mean = [results_acc[section][0] for section in sections]\n",
    "    hits_100_std = [results_rank[section][1] for section in sections]\n",
    "    # Plotting line plots for Hits@100\n",
    "    ax1.plot(x_values, hits_100_mean, marker=markers[model_name], color=colors[model_name], linestyle='-', label=model_name_dict[model_name])\n",
    "    \n",
    "# Set x-axis to a logarithmic scale\n",
    "plt.xscale('log')\n",
    "plt.xticks(x_tick_labels, labels=[f'$10^{i}$' for i in range(len(x_tick_labels))])\n",
    "\n",
    "# remove minor ticks\n",
    "plt.tick_params(axis='x', which='minor', length=0)\n",
    "\n",
    "# Setting the x-axis label\n",
    "plt.xlabel('Joint frequency of answer pair')\n",
    "# Setting the y-axis label for the first y-axis\n",
    "ax1.set_ylabel('Accuracy', color='black')\n",
    "# ax1.set_ylabel('Rank', color='black')\n",
    "# Set the limits for the y-axis if necessary\n",
    "ax1.set_ylim(0, 1)\n",
    "\n",
    "# Adding a legend for the line plots\n",
    "ax1.legend()\n",
    "\n",
    "# Show the plot\n",
    "# plt.title('Model Performance Comparison')\n",
    "filename = f'results/analogy_test_roberta_zeroshot_accuracy_against_jointprob.pdf'\n",
    "plt.tight_layout()  # Adjust layout to fit all labels\n",
    "plt.savefig(filename, format='pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_dict = {\n",
    "    # 'bert-base-uncased': 'BERT$_{base}$',\n",
    "    # 'bert-large-uncased': 'BERT$_{large}$',\n",
    "    'albert-base-v1': 'ALBERT1$_{base}$',\n",
    "    'albert-large-v1': 'ALBERT1$_{large}$',\n",
    "    'albert-xlarge-v1': 'ALBERT1$_{xlarge}$',\n",
    "    # 'albert-base-v2': 'ALBERT2$_{base}$',\n",
    "    # 'albert-large-v2': 'ALBERT2$_{large}$',\n",
    "    # 'albert-xlarge-v2': 'ALBERT2$_{xlarge}$',\n",
    "    # 'roberta-base': 'RoBERTa$_{base}$',\n",
    "    # 'roberta-large': 'RoBERTa$_{large}$',\n",
    "    # 'gpt-neo-125m': 'GPT-Neo 125M',\n",
    "    # 'gpt-neo-1.3B': 'GPT-Neo 1.3B',\n",
    "    # 'gpt-neo-2.7B': 'GPT-Neo 2.7B',\n",
    "    # 'gpt-j-6b': 'GPT-J 6B',\n",
    "    # 'gpt-3.5-turbo-0125': 'ChatGPT-3.5',\n",
    "    # 'gpt-4-0125-preview': 'ChatGPT-4'\n",
    "}\n",
    "\n",
    "colors = {\n",
    "    'albert-base-v1': 'tab:blue',\n",
    "    'albert-large-v1': 'tab:green',\n",
    "    'albert-xlarge-v1': 'tab:red',\n",
    "}\n",
    "markers = {\n",
    "    'albert-base-v1': 'o',\n",
    "    'albert-large-v1': '^',\n",
    "    'albert-xlarge-v1': 's',\n",
    "}\n",
    "\n",
    "# Scale factor for fonts\n",
    "scale_factor = 1.5\n",
    "\n",
    "# Update default font sizes\n",
    "plt.rcParams.update({\n",
    "    'font.size': 12 * scale_factor,\n",
    "    'axes.labelsize': 14 * scale_factor,  # x and y labels from plt.xlabel and plt.ylabel\n",
    "    'axes.titlesize': 16 * scale_factor,  # title from plt.title\n",
    "    'xtick.labelsize': 12 * scale_factor,  # x tick labels\n",
    "    'ytick.labelsize': 12 * scale_factor,  # y tick labels\n",
    "    'legend.fontsize': 12 * scale_factor,  # legend font size\n",
    "    'figure.titlesize': 18 * scale_factor  # suptitle\n",
    "})\n",
    "\n",
    "# Fixed x-axis values - the positions where the x-tick labels will be placed\n",
    "x_tick_labels = [1, 10, 100, 1000, 10000, 100000, 1000000, 10000000]\n",
    "# x_tick_labels = [1, 10, 100, 1000, 10000, 100000, 1000000]\n",
    "# x_tick_labels = [1, 10, 100, 1000, 10000]\n",
    "# Calculate midpoints for plotting the data points\n",
    "x_values = np.sqrt(np.array(x_tick_labels[:-1]) * np.array(x_tick_labels[1:]))\n",
    "\n",
    "# Create a figure and a set of subplots\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for model_name in model_name_dict.keys():\n",
    "    print('='*30)\n",
    "    print('='*30)\n",
    "    print('Model:', model_name)\n",
    "\n",
    "    try:\n",
    "        data = json.load(open(f'results/{model_name}_pred.json', 'r'))\n",
    "    except:\n",
    "        raise Exception\n",
    "        # continue\n",
    "\n",
    "    if 'gpt' in model_name or 'Llama' in model_name:\n",
    "        coo_matrix = pile_coo_matrix\n",
    "        num_total_samples = 254188957\n",
    "    else:\n",
    "        coo_matrix = bert_coo_matrix\n",
    "        num_total_samples = 158887337\n",
    "    # coo_matrix = bert_coo_matrix\n",
    "    # num_total_samples = 158887337\n",
    "\n",
    "    openai_api = True if 'gpt-3.5-turbo' in model_name or 'gpt-4o' in model_name else False\n",
    "\n",
    "    results_rank, results_acc, results_tail_rank, results_tail_acc = defaultdict(list), defaultdict(list), defaultdict(list), defaultdict(list)\n",
    "\n",
    "    for pred in tqdm(data):\n",
    "        uid = pred['uid']\n",
    "        if uid not in gt_filtered_uids:\n",
    "            continue\n",
    "        \n",
    "        flag = True\n",
    "        choices = pred['choice']\n",
    "        freqs = []\n",
    "        for c_idx, choice in enumerate(choices):\n",
    "            subj, obj = choice\n",
    "            subj = ' '.join(text_normalization_without_lemmatization(subj))\n",
    "            obj = ' '.join(text_normalization_without_lemmatization(obj))\n",
    "\n",
    "            subj_count = coo_matrix.count(subj)\n",
    "            obj_count = coo_matrix.count(obj)\n",
    "            subj_obj_count = coo_matrix.coo_count(subj, obj)\n",
    "            \n",
    "            if subj_obj_count < 0:\n",
    "                flag = False\n",
    "                break\n",
    "\n",
    "            freq = subj_obj_count\n",
    "            freqs.append(freq)\n",
    "        \n",
    "        if not flag:\n",
    "            continue\n",
    "\n",
    "        ans_freq = freqs[pred['answer']]\n",
    "        freq_order = np.argsort(freqs).tolist()\n",
    "        freq_rank = freq_order.index(pred['answer'])\n",
    "\n",
    "        section = frequency_to_section(ans_freq)\n",
    "\n",
    "        results_rank[section].append(1 + pred['rank'])\n",
    "        results_acc[section].append(pred['accuracy'])\n",
    "        results_tail_rank[section].append(1 + pred['tail_rank'])\n",
    "        results_tail_acc[section].append(pred['tail_accuracy'])\n",
    "\n",
    "    num_samples = {}\n",
    "    sections = range(1, len(bins)+1)\n",
    "    for section in sections:\n",
    "        num_samples[section] = len(results_rank[section])\n",
    "\n",
    "        if section in results_rank:\n",
    "            results_rank[section] = np.mean(results_rank[section]), np.std(results_rank[section])\n",
    "            results_acc[section] = np.mean(results_acc[section]), np.std(results_acc[section])\n",
    "            results_tail_rank[section] = np.mean(results_tail_rank[section]), np.std(results_tail_rank[section])\n",
    "            results_tail_acc[section] = np.mean(results_tail_acc[section]), np.std(results_tail_acc[section])\n",
    "\n",
    "    result = {}\n",
    "    for section in sections:\n",
    "        if section in results_rank:\n",
    "            result[f'result_rank_section_{frequency_section_to_string(section)}'] = f'%.2f +- %.2f' % results_rank[section]\n",
    "    for section in sections:\n",
    "        if section in results_acc:\n",
    "            result[f'result_accuracy_section_{frequency_section_to_string(section)}'] = f'%.2f +- %.2f' % results_acc[section]\n",
    "    for section in sections:\n",
    "        if section in results_tail_rank:\n",
    "            result[f'result_tail_rank_section_{frequency_section_to_string(section)}'] = f'%.2f +- %.2f' % results_tail_rank[section]\n",
    "    for section in sections:\n",
    "        if section in results_tail_acc:\n",
    "            result[f'result_tail_accuracy_section_{frequency_section_to_string(section)}'] = f'%.2f +- %.2f' % results_tail_acc[section]\n",
    "            \n",
    "    print(num_samples)\n",
    "\n",
    "    hits_100_mean = [results_acc[section][0] for section in sections]\n",
    "    hits_100_std = [results_rank[section][1] for section in sections]\n",
    "    # Plotting line plots for Hits@100\n",
    "    ax1.plot(x_values, hits_100_mean, marker=markers[model_name], color=colors[model_name], linestyle='-', label=model_name_dict[model_name])\n",
    "    \n",
    "# Set x-axis to a logarithmic scale\n",
    "plt.xscale('log')\n",
    "plt.xticks(x_tick_labels, labels=[f'$10^{i}$' for i in range(len(x_tick_labels))])\n",
    "\n",
    "# remove minor ticks\n",
    "plt.tick_params(axis='x', which='minor', length=0)\n",
    "\n",
    "# Setting the x-axis label\n",
    "plt.xlabel('Joint frequency of answer pair')\n",
    "# Setting the y-axis label for the first y-axis\n",
    "ax1.set_ylabel('Accuracy', color='black')\n",
    "# ax1.set_ylabel('Rank', color='black')\n",
    "# Set the limits for the y-axis if necessary\n",
    "ax1.set_ylim(0, 1)\n",
    "\n",
    "# Adding a legend for the line plots\n",
    "ax1.legend()\n",
    "\n",
    "# Show the plot\n",
    "# plt.title('Model Performance Comparison')\n",
    "filename = f'results/analogy_test_albert1_zeroshot_accuracy_against_jointprob.pdf'\n",
    "plt.tight_layout()  # Adjust layout to fit all labels\n",
    "plt.savefig(filename, format='pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_dict = {\n",
    "    # 'bert-base-uncased': 'BERT$_{base}$',\n",
    "    # 'bert-large-uncased': 'BERT$_{large}$',\n",
    "    # 'albert-base-v1': 'ALBERT1$_{base}$',\n",
    "    # 'albert-large-v1': 'ALBERT1$_{large}$',\n",
    "    # 'albert-xlarge-v1': 'ALBERT1$_{xlarge}$',\n",
    "    'albert-base-v2': 'ALBERT2$_{base}$',\n",
    "    'albert-large-v2': 'ALBERT2$_{large}$',\n",
    "    'albert-xlarge-v2': 'ALBERT2$_{xlarge}$',\n",
    "    # 'roberta-base': 'RoBERTa$_{base}$',\n",
    "    # 'roberta-large': 'RoBERTa$_{large}$',\n",
    "    # 'gpt-neo-125m': 'GPT-Neo 125M',\n",
    "    # 'gpt-neo-1.3B': 'GPT-Neo 1.3B',\n",
    "    # 'gpt-neo-2.7B': 'GPT-Neo 2.7B',\n",
    "    # 'gpt-j-6b': 'GPT-J 6B',\n",
    "    # 'gpt-3.5-turbo-0125': 'ChatGPT-3.5',\n",
    "    # 'gpt-4-0125-preview': 'ChatGPT-4'\n",
    "}\n",
    "\n",
    "colors = {\n",
    "    'albert-base-v2': 'tab:blue',\n",
    "    'albert-large-v2': 'tab:green',\n",
    "    'albert-xlarge-v2': 'tab:red',\n",
    "}\n",
    "markers = {\n",
    "    'albert-base-v2': 'o',\n",
    "    'albert-large-v2': '^',\n",
    "    'albert-xlarge-v2': 's',\n",
    "}\n",
    "\n",
    "# Scale factor for fonts\n",
    "scale_factor = 1.5\n",
    "\n",
    "# Update default font sizes\n",
    "plt.rcParams.update({\n",
    "    'font.size': 12 * scale_factor,\n",
    "    'axes.labelsize': 14 * scale_factor,  # x and y labels from plt.xlabel and plt.ylabel\n",
    "    'axes.titlesize': 16 * scale_factor,  # title from plt.title\n",
    "    'xtick.labelsize': 12 * scale_factor,  # x tick labels\n",
    "    'ytick.labelsize': 12 * scale_factor,  # y tick labels\n",
    "    'legend.fontsize': 12 * scale_factor,  # legend font size\n",
    "    'figure.titlesize': 18 * scale_factor  # suptitle\n",
    "})\n",
    "\n",
    "# Fixed x-axis values - the positions where the x-tick labels will be placed\n",
    "x_tick_labels = [1, 10, 100, 1000, 10000, 100000, 1000000, 10000000]\n",
    "# x_tick_labels = [1, 10, 100, 1000, 10000, 100000, 1000000]\n",
    "# x_tick_labels = [1, 10, 100, 1000, 10000]\n",
    "# Calculate midpoints for plotting the data points\n",
    "x_values = np.sqrt(np.array(x_tick_labels[:-1]) * np.array(x_tick_labels[1:]))\n",
    "\n",
    "# Create a figure and a set of subplots\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for model_name in model_name_dict.keys():\n",
    "    print('='*30)\n",
    "    print('='*30)\n",
    "    print('Model:', model_name)\n",
    "\n",
    "    try:\n",
    "        data = json.load(open(f'results/{model_name}_pred.json', 'r'))\n",
    "    except:\n",
    "        raise Exception\n",
    "        # continue\n",
    "\n",
    "    if 'gpt' in model_name or 'Llama' in model_name:\n",
    "        coo_matrix = pile_coo_matrix\n",
    "        num_total_samples = 254188957\n",
    "    else:\n",
    "        coo_matrix = bert_coo_matrix\n",
    "        num_total_samples = 158887337\n",
    "    # coo_matrix = bert_coo_matrix\n",
    "    # num_total_samples = 158887337\n",
    "\n",
    "    openai_api = True if 'gpt-3.5-turbo' in model_name or 'gpt-4o' in model_name else False\n",
    "\n",
    "    results_rank, results_acc, results_tail_rank, results_tail_acc = defaultdict(list), defaultdict(list), defaultdict(list), defaultdict(list)\n",
    "\n",
    "    for pred in tqdm(data):\n",
    "        uid = pred['uid']\n",
    "        if uid not in gt_filtered_uids:\n",
    "            continue\n",
    "\n",
    "        flag = True\n",
    "        choices = pred['choice']\n",
    "        freqs = []\n",
    "        for c_idx, choice in enumerate(choices):\n",
    "            subj, obj = choice\n",
    "            subj = ' '.join(text_normalization_without_lemmatization(subj))\n",
    "            obj = ' '.join(text_normalization_without_lemmatization(obj))\n",
    "\n",
    "            subj_count = coo_matrix.count(subj)\n",
    "            obj_count = coo_matrix.count(obj)\n",
    "            subj_obj_count = coo_matrix.coo_count(subj, obj)\n",
    "            \n",
    "            if subj_obj_count < 0:\n",
    "                flag = False\n",
    "                break\n",
    "\n",
    "            freq = subj_obj_count\n",
    "            freqs.append(freq)\n",
    "        \n",
    "        if not flag:\n",
    "            continue\n",
    "\n",
    "        ans_freq = freqs[pred['answer']]\n",
    "        freq_order = np.argsort(freqs).tolist()\n",
    "        freq_rank = freq_order.index(pred['answer'])\n",
    "\n",
    "        section = frequency_to_section(ans_freq)\n",
    "\n",
    "        results_rank[section].append(1 + pred['rank'])\n",
    "        results_acc[section].append(pred['accuracy'])\n",
    "        results_tail_rank[section].append(1 + pred['tail_rank'])\n",
    "        results_tail_acc[section].append(pred['tail_accuracy'])\n",
    "\n",
    "    num_samples = {}\n",
    "    sections = range(1, len(bins)+1)\n",
    "    for section in sections:\n",
    "        num_samples[section] = len(results_rank[section])\n",
    "\n",
    "        if section in results_rank:\n",
    "            results_rank[section] = np.mean(results_rank[section]), np.std(results_rank[section])\n",
    "            results_acc[section] = np.mean(results_acc[section]), np.std(results_acc[section])\n",
    "            results_tail_rank[section] = np.mean(results_tail_rank[section]), np.std(results_tail_rank[section])\n",
    "            results_tail_acc[section] = np.mean(results_tail_acc[section]), np.std(results_tail_acc[section])\n",
    "\n",
    "    result = {}\n",
    "    for section in sections:\n",
    "        if section in results_rank:\n",
    "            result[f'result_rank_section_{frequency_section_to_string(section)}'] = f'%.2f +- %.2f' % results_rank[section]\n",
    "    for section in sections:\n",
    "        if section in results_acc:\n",
    "            result[f'result_accuracy_section_{frequency_section_to_string(section)}'] = f'%.2f +- %.2f' % results_acc[section]\n",
    "    for section in sections:\n",
    "        if section in results_tail_rank:\n",
    "            result[f'result_tail_rank_section_{frequency_section_to_string(section)}'] = f'%.2f +- %.2f' % results_tail_rank[section]\n",
    "    for section in sections:\n",
    "        if section in results_tail_acc:\n",
    "            result[f'result_tail_accuracy_section_{frequency_section_to_string(section)}'] = f'%.2f +- %.2f' % results_tail_acc[section]\n",
    "            \n",
    "    print(num_samples)\n",
    "\n",
    "    hits_100_mean = [results_acc[section][0] for section in sections]\n",
    "    hits_100_std = [results_rank[section][1] for section in sections]\n",
    "    # Plotting line plots for Hits@100\n",
    "    ax1.plot(x_values, hits_100_mean, marker=markers[model_name], color=colors[model_name], linestyle='-', label=model_name_dict[model_name])\n",
    "    \n",
    "# Set x-axis to a logarithmic scale\n",
    "plt.xscale('log')\n",
    "plt.xticks(x_tick_labels, labels=[f'$10^{i}$' for i in range(len(x_tick_labels))])\n",
    "\n",
    "# remove minor ticks\n",
    "plt.tick_params(axis='x', which='minor', length=0)\n",
    "\n",
    "# Setting the x-axis label\n",
    "plt.xlabel('Joint frequency of answer pair')\n",
    "# Setting the y-axis label for the first y-axis\n",
    "ax1.set_ylabel('Accuracy', color='black')\n",
    "# ax1.set_ylabel('Rank', color='black')\n",
    "# Set the limits for the y-axis if necessary\n",
    "ax1.set_ylim(0, 1)\n",
    "\n",
    "# Adding a legend for the line plots\n",
    "ax1.legend()\n",
    "\n",
    "# Show the plot\n",
    "# plt.title('Model Performance Comparison')\n",
    "filename = f'results/analogy_test_albert2_zeroshot_accuracy_against_jointprob.pdf'\n",
    "plt.tight_layout()  # Adjust layout to fit all labels\n",
    "plt.savefig(filename, format='pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [0, 10, 100, 1000, 10000, 100000, 1000000]\n",
    "# bins = [0, 10, 100, 1000, 10000, 100000]\n",
    "# bins = [0, 10, 100, 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_dict = {\n",
    "    'bert-base-uncased': 'BERT$_{base}$',\n",
    "    'bert-large-uncased': 'BERT$_{large}$',\n",
    "    'gpt-j-6b': 'GPT-J 6B',\n",
    "    'Meta-Llama-3-8B': 'Llama-3 8B',\n",
    "}\n",
    "\n",
    "colors = {\n",
    "    'bert-base-uncased': 'tab:blue',\n",
    "    'bert-large-uncased': 'tab:green',\n",
    "    'gpt-j-6b': 'tab:red',\n",
    "    'Meta-Llama-3-8B': 'tab:orange',\n",
    "}\n",
    "markers = {\n",
    "    'bert-base-uncased': 'o',\n",
    "    'bert-large-uncased': '^',\n",
    "    'gpt-j-6b': 's',\n",
    "    'Meta-Llama-3-8B': 'D',\n",
    "}\n",
    "\n",
    "# Scale factor for fonts\n",
    "scale_factor = 1.5\n",
    "\n",
    "# Update default font sizes\n",
    "plt.rcParams.update({\n",
    "    'font.size': 12 * scale_factor,\n",
    "    'axes.labelsize': 14 * scale_factor,  # x and y labels from plt.xlabel and plt.ylabel\n",
    "    'axes.titlesize': 16 * scale_factor,  # title from plt.title\n",
    "    'xtick.labelsize': 12 * scale_factor,  # x tick labels\n",
    "    'ytick.labelsize': 12 * scale_factor,  # y tick labels\n",
    "    'legend.fontsize': 12 * scale_factor,  # legend font size\n",
    "    'figure.titlesize': 18 * scale_factor  # suptitle\n",
    "})\n",
    "\n",
    "# Fixed x-axis values - the positions where the x-tick labels will be placed\n",
    "x_tick_labels = [1, 10, 100, 1000, 10000, 100000, 1000000, 10000000]\n",
    "# x_tick_labels = [1, 10, 100, 1000, 10000, 100000, 1000000]\n",
    "x_tick_labels = [1, 2, 3, 4, 5]\n",
    "# Calculate midpoints for plotting the data points\n",
    "x_values = np.sqrt(np.array(x_tick_labels[:-1]) * np.array(x_tick_labels[1:]))\n",
    "x_values = x_tick_labels\n",
    "\n",
    "# Create a figure and a set of subplots\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for model_name in model_name_dict.keys():\n",
    "    print('='*30)\n",
    "    print('='*30)\n",
    "    print('Model:', model_name)\n",
    "\n",
    "    try:\n",
    "        data = json.load(open(f'results/{model_name}_pred.json', 'r'))\n",
    "    except:\n",
    "        raise Exception\n",
    "        # continue\n",
    "\n",
    "    if 'gpt' in model_name or 'Llama' in model_name:\n",
    "        coo_matrix = pile_coo_matrix\n",
    "        num_total_samples = 254188957\n",
    "    else:\n",
    "        coo_matrix = bert_coo_matrix\n",
    "        num_total_samples = 158887337\n",
    "    # coo_matrix = bert_coo_matrix\n",
    "    # num_total_samples = 158887337\n",
    "\n",
    "    openai_api = True if 'gpt-3.5-turbo' in model_name or 'gpt-4o' in model_name else False\n",
    "\n",
    "    results_rank, results_acc, results_tail_rank, results_tail_acc = defaultdict(list), defaultdict(list), defaultdict(list), defaultdict(list)\n",
    "\n",
    "    for pred in tqdm(data):\n",
    "        uid = pred['uid']\n",
    "        if uid not in gt_filtered_uids:\n",
    "            continue\n",
    "\n",
    "        flag = True\n",
    "        choices = pred['choice']\n",
    "        freqs = []\n",
    "        for c_idx, choice in enumerate(choices):\n",
    "            subj, obj = choice\n",
    "            subj = ' '.join(text_normalization_without_lemmatization(subj))\n",
    "            obj = ' '.join(text_normalization_without_lemmatization(obj))\n",
    "\n",
    "            subj_count = coo_matrix.count(subj)\n",
    "            obj_count = coo_matrix.count(obj)\n",
    "            subj_obj_count = coo_matrix.coo_count(subj, obj)\n",
    "            \n",
    "            if subj_obj_count < 0:\n",
    "                flag = False\n",
    "                break\n",
    "\n",
    "            freq = subj_obj_count\n",
    "            freqs.append(freq)\n",
    "        \n",
    "        if not flag:\n",
    "            continue\n",
    "\n",
    "        ans_freq = freqs[pred['answer']]\n",
    "        freq_order = np.argsort(freqs).tolist()\n",
    "        freq_rank = freq_order.index(pred['answer'])\n",
    "        freq_rank = len(choices)-1 - freq_rank\n",
    "\n",
    "        # section = frequency_to_section(ans_freq)\n",
    "        section = frequency_to_section(10**freq_rank)\n",
    "\n",
    "        results_rank[section].append(pred['rank'] + 1)\n",
    "        results_acc[section].append(pred['accuracy'])\n",
    "        results_tail_rank[section].append(pred['tail_rank'] + 1)\n",
    "        results_tail_acc[section].append(pred['tail_accuracy'])\n",
    "\n",
    "    num_samples = {}\n",
    "    sections = range(1, len(bins)+1)\n",
    "    sections = range(1, 5+1)\n",
    "    for section in sections:\n",
    "        num_samples[section] = len(results_rank[section])\n",
    "\n",
    "        if section in results_rank:\n",
    "            results_rank[section] = np.mean(results_rank[section]), np.std(results_rank[section])\n",
    "            results_acc[section] = np.mean(results_acc[section]), np.std(results_acc[section])\n",
    "            results_tail_rank[section] = np.mean(results_tail_rank[section]), np.std(results_tail_rank[section])\n",
    "            results_tail_acc[section] = np.mean(results_tail_acc[section]), np.std(results_tail_acc[section])\n",
    "\n",
    "    result = {}\n",
    "    for section in sections:\n",
    "        if section in results_rank:\n",
    "            result[f'result_rank_section_{frequency_section_to_string(section)}'] = f'%.2f +- %.2f' % results_rank[section]\n",
    "    for section in sections:\n",
    "        if section in results_acc:\n",
    "            result[f'result_accuracy_section_{frequency_section_to_string(section)}'] = f'%.2f +- %.2f' % results_acc[section]\n",
    "    for section in sections:\n",
    "        if section in results_tail_rank:\n",
    "            result[f'result_tail_rank_section_{frequency_section_to_string(section)}'] = f'%.2f +- %.2f' % results_tail_rank[section]\n",
    "    for section in sections:\n",
    "        if section in results_tail_acc:\n",
    "            result[f'result_tail_accuracy_section_{frequency_section_to_string(section)}'] = f'%.2f +- %.2f' % results_tail_acc[section]\n",
    "            \n",
    "    print(num_samples)\n",
    "\n",
    "    hits_100_mean = [results_acc[section][0] for section in sections]\n",
    "    hits_100_std = [results_rank[section][1] for section in sections]\n",
    "    # Plotting line plots for Hits@100\n",
    "    ax1.plot(x_values, hits_100_mean, marker=markers[model_name], color=colors[model_name], linestyle='-', label=model_name_dict[model_name])\n",
    "    \n",
    "# Set x-axis to a logarithmic scale\n",
    "# plt.xscale('log')\n",
    "plt.xticks(x_tick_labels, labels=[f'${i+1}$' for i in range(len(x_tick_labels))])\n",
    "\n",
    "# remove minor ticks\n",
    "plt.tick_params(axis='x', which='minor', length=0)\n",
    "\n",
    "# Setting the x-axis label\n",
    "# plt.xscale('log')\n",
    "plt.xlabel('Rank of joint frequency of answer pair')\n",
    "# Setting the y-axis label for the first y-axis\n",
    "ax1.set_ylabel('Accuracy', color='black')\n",
    "# Set the limits for the y-axis if necessary\n",
    "ax1.set_ylim(0, 1)\n",
    "\n",
    "# Adding a legend for the line plots\n",
    "ax1.legend()\n",
    "\n",
    "# Show the plot\n",
    "# plt.title('Model Performance Comparison')\n",
    "filename = f'results/analogy_test_zeroshot_accuracy_against_jointprob_rank.pdf'\n",
    "plt.tight_layout()  # Adjust layout to fit all labels\n",
    "plt.savefig(filename, format='pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_type = 'zeroshot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "model_name_dict = {\n",
    "    # 'bert-base-uncased': 'BERT$_{base}$',\n",
    "    # 'Meta-Llama-3-8B': 'Llama-3 8B',\n",
    "    # 'gpt-3.5-turbo-0125': 'z'\n",
    "    # 'gpt-4o-2024-08-06': 'z'\n",
    "    'bert-large-uncased': 'z'\n",
    "}\n",
    "\n",
    "# Scale factor for fonts\n",
    "scale_factor = 1.5\n",
    "\n",
    "# Update default font sizes\n",
    "plt.rcParams.update({\n",
    "    'font.size': 12 * scale_factor,\n",
    "    'axes.labelsize': 14 * scale_factor,  # x and y labels from plt.xlabel and plt.ylabel\n",
    "    'axes.titlesize': 16 * scale_factor,  # title from plt.title\n",
    "    'xtick.labelsize': 12 * scale_factor,  # x tick labels\n",
    "    'ytick.labelsize': 12 * scale_factor,  # y tick labels\n",
    "    'legend.fontsize': 12 * scale_factor,  # legend font size\n",
    "    'figure.titlesize': 18 * scale_factor  # suptitle\n",
    "})\n",
    "\n",
    "joint_freq_bins = [f'$10^{i}$' for i in range(6+1)]\n",
    "subject_freq_bins = [f'$10^{i+1}$' for i in range(6)]\n",
    "\n",
    "# Create a figure and a set of subplots\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for model_name in model_name_dict.keys():\n",
    "    print('='*30)\n",
    "    print('='*30)\n",
    "    print('Model:', model_name)\n",
    "\n",
    "    try:\n",
    "        data = json.load(open(f'results/{model_name}_pred.json', 'r'))\n",
    "    except:\n",
    "        raise Exception\n",
    "        # continue\n",
    "\n",
    "    if 'gpt' in model_name or 'Llama' in model_name:\n",
    "        coo_matrix = pile_coo_matrix\n",
    "        num_total_samples = 254188957\n",
    "    else:\n",
    "        coo_matrix = bert_coo_matrix\n",
    "        num_total_samples = 158887337\n",
    "    # coo_matrix = bert_coo_matrix\n",
    "    # num_total_samples = 158887337\n",
    "\n",
    "    openai_api = True if 'gpt-3.5-turbo' in model_name or 'gpt-4o' in model_name else False\n",
    "\n",
    "    results_rank, results_acc, results_tail_rank, results_tail_acc = defaultdict(list), defaultdict(list), defaultdict(list), defaultdict(list)\n",
    "\n",
    "    for pred in tqdm(data):\n",
    "        uid = pred['uid']\n",
    "        if uid not in gt_filtered_uids:\n",
    "            continue\n",
    "\n",
    "        flag = True\n",
    "        choices = pred['choice']\n",
    "        subj_obj_counts = []\n",
    "        subj_counts = []\n",
    "        for c_idx, choice in enumerate(choices):\n",
    "            subj, obj = choice\n",
    "            subj = ' '.join(text_normalization_without_lemmatization(subj))\n",
    "            obj = ' '.join(text_normalization_without_lemmatization(obj))\n",
    "\n",
    "            subj_count = coo_matrix.count(subj)\n",
    "            obj_count = coo_matrix.count(obj)\n",
    "            subj_obj_count = coo_matrix.coo_count(subj, obj)\n",
    "            \n",
    "            if subj_obj_count < 0:\n",
    "                flag = False\n",
    "                break\n",
    "\n",
    "            subj_obj_counts.append(subj_obj_count)\n",
    "            subj_counts.append(subj_count)\n",
    "        \n",
    "        if not flag:\n",
    "            continue\n",
    "\n",
    "        ans_subj_obj_count = subj_obj_counts[pred['answer']]\n",
    "        ans_subj_count = subj_counts[pred['answer']]\n",
    "\n",
    "        subj_prob = ans_subj_count / num_total_samples\n",
    "        joint_prob = ans_subj_obj_count / num_total_samples\n",
    "        cond_prob = ans_subj_obj_count / ans_subj_count if ans_subj_count > 0 else 0\n",
    "\n",
    "        joint_freq = ans_subj_obj_count\n",
    "        joint_section = frequency_to_section(joint_freq)\n",
    "\n",
    "        subj_freq = ans_subj_count\n",
    "        subj_section = frequency_to_section(subj_freq)\n",
    "\n",
    "        section = f'{joint_section}_{subj_section}'\n",
    "\n",
    "        # results_rank[section].append(pred['rank'])\n",
    "        results_acc[section].append(pred['accuracy'])\n",
    "        # results_tail_rank[section].append(pred['tail_rank'])\n",
    "        # results_tail_acc[section].append(pred['tail_accuracy'])\n",
    "\n",
    "    num_samples = {}\n",
    "    joint_sections = range(1, len(bins)+1)\n",
    "    subj_sections = range(1, len(bins)+1)\n",
    "    for joint_section in joint_sections:\n",
    "        for subj_section in subj_sections:\n",
    "            section = f'{joint_section}_{subj_section}'\n",
    "            num_samples[section] = len(results_acc[section])\n",
    "\n",
    "            if section in results_acc:\n",
    "                # results_rank[section] = np.mean(results_rank[section]), np.std(results_rank[section])\n",
    "                results_acc[section] = np.mean(results_acc[section]), np.std(results_acc[section])\n",
    "                # results_tail_rank[section] = np.mean(results_tail_rank[section]), np.std(results_tail_rank[section])\n",
    "                # results_tail_acc[section] = np.mean(results_tail_acc[section]), np.std(results_tail_acc[section])\n",
    "\n",
    "    result = {}\n",
    "    # for joint_section in joint_sections:\n",
    "    #     for subj_section in subj_sections:\n",
    "    #         section = f'{joint_section}_{subj_section}'\n",
    "    #         if section in results_rank:\n",
    "    #             result[f'result_rank_section_{frequency_section_to_string(section)}'] = f'%.2f +- %.2f' % results_rank[section]\n",
    "    for joint_section in joint_sections:\n",
    "        for subj_section in subj_sections:\n",
    "            section = f'{joint_section}_{subj_section}'\n",
    "            if section in results_acc:\n",
    "                result[f'result_accuracy_section_{frequency_section_to_string(section)}'] = f'%.2f +- %.2f' % results_acc[section]\n",
    "    # for joint_section in joint_sections:\n",
    "    #     for subj_section in subj_sections:\n",
    "    #         section = f'{joint_section}_{subj_section}'\n",
    "    #         if section in results_tail_rank:\n",
    "    #             result[f'result_tail_rank_section_{frequency_section_to_string(section)}'] = f'%.2f +- %.2f' % results_tail_rank[section]\n",
    "    # for joint_section in joint_sections:\n",
    "    #     for subj_section in subj_sections:\n",
    "    #         section = f'{joint_section}_{subj_section}'\n",
    "    #         if section in results_tail_acc:\n",
    "    #             result[f'result_tail_accuracy_section_{frequency_section_to_string(section)}'] = f'%.2f +- %.2f' % results_tail_acc[section]\n",
    "\n",
    "    # for joint_section in joint_sections:\n",
    "    #     for subj_section in subj_sections:\n",
    "    #         section = f'{joint_section}_{subj_section}'\n",
    "    #         if section in results_hits_100:\n",
    "    #             result[f'hits@100_remove_stopwords_section_{section}'] = f'%.2f +- %.2f' % results_hits_100[section]\n",
    "\n",
    "    print(num_samples)\n",
    "\n",
    "    hits_100_mean = [[results_acc[f'{joint_section}_{subj_section}'][0] for joint_section in joint_sections] for subj_section in subj_sections]\n",
    "    hits_100_std = [[results_acc[f'{joint_section}_{subj_section}'][1] for joint_section in joint_sections] for subj_section in subj_sections]\n",
    "\n",
    "    data = np.array(hits_100_mean)\n",
    "\n",
    "    mask = np.ones_like(data.T, dtype='bool')\n",
    "    mask[np.triu_indices_from(mask)] = False\n",
    "    mask = np.rot90(mask, 1)\n",
    "\n",
    "    data = np.flipud(data)\n",
    "\n",
    "    ax = sns.heatmap(data, mask=mask, annot=True, fmt=\".2f\", linewidth=0.5, cmap='Blues',\n",
    "                     cbar_kws={'label': 'Accuracy'})\n",
    "    ax.set_facecolor(\"white\")\n",
    "    \n",
    "# Rotate the tick labels for clarity\n",
    "plt.xticks(range(len(joint_freq_bins)), joint_freq_bins, rotation=0, ha='right')\n",
    "plt.yticks(range(len(subject_freq_bins)), subject_freq_bins[::-1], rotation=0)\n",
    "\n",
    "# Set axis labels and title\n",
    "plt.xlabel('Joint frequency of answer pair')\n",
    "plt.ylabel('Marginal frequency of answer head')\n",
    "\n",
    "# Show the plot\n",
    "# plt.title('Model Performance Comparison')\n",
    "filename = f'results/{model_name}_{training_type}_accuracy_against_condprob.pdf'\n",
    "plt.tight_layout()  # Adjust layout to fit all labels\n",
    "plt.savefig(filename, format='pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_type = 'zeroshot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "model_name_dict = {\n",
    "    'bert-base-uncased': 'BERT$_{base}$',\n",
    "    # 'Meta-Llama-3-8B': 'Llama-3 8B',\n",
    "}\n",
    "\n",
    "# Scale factor for fonts\n",
    "scale_factor = 1.5\n",
    "\n",
    "# Update default font sizes\n",
    "plt.rcParams.update({\n",
    "    'font.size': 12 * scale_factor,\n",
    "    'axes.labelsize': 14 * scale_factor,  # x and y labels from plt.xlabel and plt.ylabel\n",
    "    'axes.titlesize': 16 * scale_factor,  # title from plt.title\n",
    "    'xtick.labelsize': 12 * scale_factor,  # x tick labels\n",
    "    'ytick.labelsize': 12 * scale_factor,  # y tick labels\n",
    "    'legend.fontsize': 12 * scale_factor,  # legend font size\n",
    "    'figure.titlesize': 18 * scale_factor  # suptitle\n",
    "})\n",
    "\n",
    "joint_freq_bins = [f'$10^{i}$' for i in range(6+1)]\n",
    "subject_freq_bins = [f'$10^{i+1}$' for i in range(6)]\n",
    "\n",
    "# Create a figure and a set of subplots\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for model_name in model_name_dict.keys():\n",
    "    print('='*30)\n",
    "    print('='*30)\n",
    "    print('Model:', model_name)\n",
    "\n",
    "    try:\n",
    "        data = jsonlines.open(f'../../../results/{dataset_name}/{model_name}_{dataset_name}_{training_type}/pred_{dataset_name}_{dataset_type}.jsonl')\n",
    "    except:\n",
    "        raise Exception\n",
    "        # continue\n",
    "\n",
    "    if 'gpt' in model_name or 'Llama' in model_name:\n",
    "        coo_matrix = pile_coo_matrix\n",
    "        num_total_samples = 254188957\n",
    "    else:\n",
    "        coo_matrix = bert_coo_matrix\n",
    "        num_total_samples = 158887337\n",
    "\n",
    "    openai_api = True if 'gpt-3.5-turbo' in model_name or 'gpt-4o' in model_name else False\n",
    "\n",
    "    results_hits_1, results_hits_10, results_hits_100 = defaultdict(list), defaultdict(list), defaultdict(list)\n",
    "    rel_results_hits_1, rel_results_hits_10, rel_results_hits_100 = defaultdict(dict), defaultdict(dict), defaultdict(dict)\n",
    "\n",
    "    for pred in tqdm(data.iter()):\n",
    "        subj = uid_subj_map[pred['uid']]\n",
    "        rel = uid_rel_map[pred['uid']]\n",
    "        obj = uid_obj_map[pred['uid']]\n",
    "        subj = ' '.join(text_normalization_without_lemmatization(subj))\n",
    "        obj = ' '.join(text_normalization_without_lemmatization(obj))\n",
    "        \n",
    "        subj_count = coo_matrix.count(subj)\n",
    "        obj_count = coo_matrix.count(obj)\n",
    "        subj_obj_count = coo_matrix.coo_count(subj, obj)\n",
    "\n",
    "        # skip if the count is -1 (unknown)\n",
    "        if subj_obj_count < 0:\n",
    "            continue\n",
    "\n",
    "        subj_prob = subj_count / num_total_samples\n",
    "        joint_prob = subj_obj_count / num_total_samples\n",
    "        cond_prob = subj_obj_count / subj_count if subj_count > 0 else 0\n",
    "\n",
    "        joint_freq = subj_obj_count\n",
    "        joint_section = frequency_to_section(joint_freq)\n",
    "\n",
    "        subj_freq = subj_count\n",
    "        subj_section = frequency_to_section(subj_freq)\n",
    "\n",
    "        section = f'{joint_section}_{subj_section}'\n",
    "\n",
    "        results_hits_1[section].append(pred['hits@1_remove_stopwords'])\n",
    "        results_hits_10[section].append(pred['hits@10_remove_stopwords'])\n",
    "        if not openai_api:\n",
    "            results_hits_100[section].append(pred['hits@100_remove_stopwords'])\n",
    "\n",
    "        # if section not in rel_results_hits_1[rel]:\n",
    "        #     rel_results_hits_1[rel][section] = []\n",
    "        #     rel_results_hits_10[rel][section] = []\n",
    "        #     rel_results_hits_100[rel][section] = []\n",
    "        # rel_results_hits_1[rel][section].append(pred['hits@1_remove_stopwords'])\n",
    "        # rel_results_hits_10[rel][section].append(pred['hits@10_remove_stopwords'])\n",
    "        # if not openai_api:\n",
    "        #     rel_results_hits_100[rel][section].append(pred['hits@100_remove_stopwords'])\n",
    "\n",
    "    num_samples = {}\n",
    "    joint_sections = range(1, len(bins)+1)\n",
    "    subj_sections = range(1, len(bins)+1)\n",
    "    for joint_section in joint_sections:\n",
    "        for subj_section in subj_sections:\n",
    "            section = f'{joint_section}_{subj_section}'\n",
    "            num_samples[section] = len(results_hits_1[section])\n",
    "\n",
    "            if section in results_hits_1:\n",
    "                results_hits_1[section] = np.mean(results_hits_1[section]), np.std(results_hits_1[section])\n",
    "                results_hits_10[section] = np.mean(results_hits_10[section]), np.std(results_hits_10[section])\n",
    "                results_hits_100[section] = np.mean(results_hits_100[section]), np.std(results_hits_100[section])\n",
    "\n",
    "    result = {}\n",
    "    for joint_section in joint_sections:\n",
    "        for subj_section in subj_sections:\n",
    "            section = f'{joint_section}_{subj_section}'\n",
    "            if section in results_hits_1:\n",
    "                result[f'hits@1_remove_stopwords_section_{section}'] = f'%.2f +- %.2f' % results_hits_1[section]\n",
    "\n",
    "    for joint_section in joint_sections:\n",
    "        for subj_section in subj_sections:\n",
    "            section = f'{joint_section}_{subj_section}'\n",
    "            if section in results_hits_100:\n",
    "                result[f'hits@100_remove_stopwords_section_{section}'] = f'%.2f +- %.2f' % results_hits_100[section]\n",
    "\n",
    "    print(num_samples)\n",
    "    # print(json.dumps(result, indent=4))\n",
    "\n",
    "    hits_100_mean = [[results_hits_100[f'{joint_section}_{subj_section}'][0] for joint_section in joint_sections] for subj_section in subj_sections]\n",
    "    hits_100_std = [[results_hits_100[f'{joint_section}_{subj_section}'][1] for joint_section in joint_sections] for subj_section in subj_sections]\n",
    "\n",
    "    data = np.array(hits_100_mean)\n",
    "\n",
    "    mask = np.ones_like(data.T, dtype='bool')\n",
    "    mask[np.triu_indices_from(mask)] = False\n",
    "    mask = np.rot90(mask, 1)\n",
    "\n",
    "    data = np.flipud(data)\n",
    "\n",
    "    ax = sns.heatmap(data, mask=mask, annot=True, fmt=\".2f\", linewidth=0.5, cmap='Blues',\n",
    "                     cbar_kws={'label': 'Hits@100'})\n",
    "    ax.set_facecolor(\"white\")\n",
    "    \n",
    "# Rotate the tick labels for clarity\n",
    "plt.xticks(range(len(joint_freq_bins)), joint_freq_bins, rotation=0, ha='right')\n",
    "plt.yticks(range(len(subject_freq_bins)), subject_freq_bins[::-1], rotation=0)\n",
    "\n",
    "# Set axis labels and title\n",
    "plt.xlabel('Joint frequency of subject and object')\n",
    "plt.ylabel('Subject frequency')\n",
    "\n",
    "# Show the plot\n",
    "# plt.title('Model Performance Comparison')\n",
    "filename = f'results/{dataset_name}_{dataset_type}_{model_name}_{training_type}_hits@100_against_condprob.pdf'\n",
    "plt.tight_layout()  # Adjust layout to fit all labels\n",
    "plt.savefig(filename, format='pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "factual_knowledge_probing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
